{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "indoor-accountability",
   "metadata": {},
   "source": [
    "# Cart-Pole with Actor Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "chinese-glenn",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "from itertools import count\n",
    "from logger import Logger\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "amino-mauritius",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyvirtualdisplay.display.Display at 0x7f654440e6d8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "virtual_display = Display(visible=0, size=(1400, 900))\n",
    "virtual_display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "protected-desert",
   "metadata": {},
   "outputs": [],
   "source": [
    "############ HYPERPARAMETERS ##############\n",
    "FRAMES = 2\n",
    "RESIZE_PIXELS = 40\n",
    "\n",
    "device = 'cuda:3'\n",
    "\n",
    "POLICY_LR = 1e-5\n",
    "Q_LR = 1e-5\n",
    "GAMMA = 0.99\n",
    "BETA = 0.01\n",
    "END_SCORE = 1000\n",
    "############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "remarkable-laptop",
   "metadata": {},
   "outputs": [],
   "source": [
    "############ ENVIRONMENT ##############\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(\"CartPole-v0\").unwrapped\n",
    "        self.env.reset()\n",
    "        \n",
    "        screen = self.env.render(mode='rgb_array').transpose((2,0,1))\n",
    "        _, self.screen_height, self.screen_width = screen.shape\n",
    "        \n",
    "        self.resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(RESIZE_PIXELS, interpolation=Image.CUBIC),\n",
    "                    T.Grayscale(),\n",
    "                    T.ToTensor()])\n",
    "        \n",
    "        world_width = self.env.x_threshold * 2\n",
    "        self.scale = self.screen_width / world_width\n",
    "        \n",
    "    def get_cart_location(self):\n",
    "        return int(self.env.state[0] * self.scale + self.screen_width / 2.0)\n",
    "        \n",
    "    def get_screen(self):\n",
    "        screen = self.env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "        \n",
    "        screen = screen[:, int(self.screen_height*0.4):int(self.screen_height * 0.8)]\n",
    "        screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "        screen = torch.from_numpy(screen)\n",
    "        return self.resize(screen).unsqueeze(0)\n",
    "    \n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "        \n",
    "    def step(self, action):\n",
    "        return self.env.step(action)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "spectacular-offense",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6bc2184311f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m '''\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mPolicyNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m135\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "#### NETWORK ##############\n",
    "'''\n",
    "For policy gradient, the network should input the raw pixels, and output the probabilities of choosing either 0 or 1\n",
    "'''\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self,h=60,w=135,device='cuda:0'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        input_channel = 2\n",
    "        hidden_channel = 64\n",
    "        hidden_channel2 = 32\n",
    "        kernel_size = 5\n",
    "        stride = 2\n",
    "        \n",
    "        self.base = nn.Sequential(\n",
    "            nn.Conv2d(input_channel, hidden_channel, kernel_size = kernel_size, stride = stride),\n",
    "            nn.BatchNorm2d(hidden_channel),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_channel, hidden_channel, kernel_size = kernel_size, stride = stride),\n",
    "            nn.BatchNorm2d(hidden_channel),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_channel, hidden_channel2, kernel_size = kernel_size, stride = stride),\n",
    "            nn.BatchNorm2d(hidden_channel2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        def conv2d_size_out(size, kernel_size = kernel_size, stride = stride):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        \n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * hidden_channel2\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(linear_input_size, 24),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(24, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = x.to(self.device)\n",
    "        x = self.base(x)\n",
    "        x = x.flatten(1)\n",
    "        x = self.head(x)\n",
    "        return F.softmax(x,dim = 1)\n",
    "    \n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self,h=60,w=135,device='cuda:0'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        input_channel = 2\n",
    "        hidden_channel = 64\n",
    "        hidden_channel2 = 32\n",
    "        kernel_size = 5\n",
    "        stride = 2\n",
    "        \n",
    "        self.base = nn.Sequential(\n",
    "            nn.Conv2d(input_channel, hidden_channel, kernel_size = kernel_size, stride = stride),\n",
    "            nn.BatchNorm2d(hidden_channel),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_channel, hidden_channel, kernel_size = kernel_size, stride = stride),\n",
    "            nn.BatchNorm2d(hidden_channel),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_channel, hidden_channel2, kernel_size = kernel_size, stride = stride),\n",
    "            nn.BatchNorm2d(hidden_channel2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        def conv2d_size_out(size, kernel_size = kernel_size, stride = stride):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        \n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * hidden_channel2\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(linear_input_size, 24),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(24, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = x.to(self.device)\n",
    "        x = self.base(x)\n",
    "        x = x.flatten(1)\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "loose-weight",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "elementary-archive",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.step(0)\n",
    "plt.figure()\n",
    "plt.imshow(env.get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy().squeeze(), cmap='gray')\n",
    "plt.title('Example extracted screen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "loose-soundtrack",
   "metadata": {},
   "outputs": [],
   "source": [
    "screen_shape = env.get_screen().shape\n",
    "policy_net = PolicyNetwork(h = screen_shape[2], w = screen_shape[3], device = device).to(device)\n",
    "\n",
    "q_net = QNetwork(h = screen_shape[2], w = screen_shape[3], device = device).to(device)\n",
    "\n",
    "policy_optimizer = optim.Adam(policy_net.parameters(), lr=POLICY_LR)\n",
    "q_optimizer = optim.Adam(q_net.parameters(), lr = Q_LR)\n",
    "\n",
    "q_mse_loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "inclusive-freeware",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5471, 0.4529]], device='cuda:3', grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_screen = env.get_screen().to(device)\n",
    "c_screen = env.get_screen().to(device)\n",
    "\n",
    "x = torch.cat((p_screen, c_screen),dim=1)\n",
    "\n",
    "print(\"Policy Network output:\",policy_net(x))\n",
    "print(\"Q Network output:\", q_net(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "three-copper",
   "metadata": {},
   "outputs": [],
   "source": [
    "HISTORY = []\n",
    "\n",
    "def discount_rewards(r):\n",
    "    discounted_r = torch.zeros(r.size())\n",
    "    running_add = 0\n",
    "    for t in reversed(range(len(r))):\n",
    "        running_add = running_add * GAMMA + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "\n",
    "    return discounted_r\n",
    "\n",
    "def run_episode(net, episode, env, logger = None):\n",
    "    net.eval()\n",
    "    state = env.reset()\n",
    "    prev_screen = env.get_screen().to(device)\n",
    "    \n",
    "    reward_sum = 0\n",
    "    xs = torch.FloatTensor([]).to(device)\n",
    "    ys = torch.FloatTensor([]).to(device)\n",
    "    rewards = torch.FloatTensor([]).to(device)\n",
    "    steps = 0\n",
    "\n",
    "    for t in count():\n",
    "        screen = env.get_screen().to(device)\n",
    "        \n",
    "        x = torch.cat((prev_screen, screen), dim=1)\n",
    "        \n",
    "        action_prob = net(x)\n",
    "\n",
    "        action = 0 if random.random() < action_prob[0][0] else 1\n",
    "\n",
    "        y = torch.FloatTensor([[1, 0]] if action == 0 else [[0, 1]]).to(device)\n",
    "        \n",
    "        xs = torch.cat([xs, x])\n",
    "        ys = torch.cat([ys, y])\n",
    "        \n",
    "        prev_screen = screen\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        rewards = torch.cat([rewards, torch.FloatTensor([[reward]]).to(device)])\n",
    "        reward_sum += reward\n",
    "        steps += 1\n",
    "    \n",
    "\n",
    "        if done or steps >= 1000:\n",
    "            adv = discount_rewards(rewards)\n",
    "            #adv = (adv - adv.mean())\n",
    "            adv = (adv - adv.mean())/(adv.std() + 1e-7)\n",
    "            loss,entropy = learn(xs, ys, adv, net)\n",
    "            print(\"[Episode {:>5}]  steps: {:>5} loss: {:>5} entropy: {:>5}\".format(episode, steps, loss, entropy))\n",
    "            if logger != None:\n",
    "                log.log_scalar(scalar=steps,episode=episode,name='duration')\n",
    "                log.log_scalar(scalar=reward_sum,episode=episode,name='reward')\n",
    "                log.log_scalar(scalar=entropy,episode=episode,name='entropy')\n",
    "                log.log_scalar(scalar=loss,episode=episode,name='loss')\n",
    "            return False\n",
    "\n",
    "def learn(x, y, adv, model):\n",
    "    model.train()\n",
    "    # Loss function, ∑ Ai*logp(yi∣xi), but we need fake lable Y due to autodiff\n",
    "    action_pred = model(x)\n",
    "    log_action_pred = torch.log(action_pred)\n",
    "    \n",
    "    entropy = -torch.sum(action_pred*log_action_pred,dim=1).mean()\n",
    "    \n",
    "    y = Variable(y, requires_grad=True)\n",
    "    adv = Variable(adv).to(device)\n",
    "    log_lik = -y * log_action_pred\n",
    "    # print(y)\n",
    "    log_lik_adv = log_lik * adv\n",
    "    loss = torch.sum(log_lik_adv, 1).mean() - BETA*entropy\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item(), entropy.item()\n",
    "\n",
    "def update_q(prev_state, prev_action, reward, state, q_net, policy_net, loss_function, optimizer):\n",
    "    \n",
    "    # q_net(s,a) = q_net(s,a) - lr * (reward + gamma * (q_net(s',a) @ policy_net(s')) - q_net(s,a))\n",
    "    \n",
    "    q_prev = q_net(prev_state)@prev_action\n",
    "    q = q_net(state).detach()\n",
    "    action_prob = policy_net(state).detach()\n",
    "    \n",
    "    loss = loss_function(q_prev,(reward + GAMMA*(q@action_prob)))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "def update_policy(state, action, reward, q_net, policy_net, optimizer):\n",
    "    \n",
    "    # policy_net = policy_net - lr * delta(log(policy_net) * (q_net(s,a) - q_net(s) @ policy_net(s)))\n",
    "    \n",
    "    action_prob = policy_net(state)\n",
    "    q = q_net(state).detach()\n",
    "    \n",
    "    q_policy = q@action_prob.detach()\n",
    "    q_actual = q@action\n",
    "    \n",
    "    adv = q_action - q_policy\n",
    "    log_action_prob = torch.log(action_prob)\n",
    "    log_lik = -action@log_action_prob\n",
    "    log_lik_adv = log_lik * adv\n",
    "    \n",
    "    entropy = -torch.sum(action_prob*action_prob,dim=1).mean()\n",
    "    \n",
    "    loss = log_lik_adv - BETA*entropy\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item(), entropy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "settled-yemen",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = f\"AC hidden=64 lr={LR} max_steps=1000 Adam fullwidth BETA={BETA}\"\n",
    "#name = \"test\"\n",
    "log = Logger(model_name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electrical-bracket",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode 10451]  steps:    88 loss: -0.16389600932598114 entropy: 0.519363522529602\n",
      "[Episode 10452]  steps:    67 loss: -0.08535914868116379 entropy: 0.4573885202407837\n",
      "[Episode 10453]  steps:    74 loss: -0.04664262756705284 entropy: 0.5364887118339539\n",
      "[Episode 10454]  steps:    79 loss: -0.10459451377391815 entropy: 0.5561109781265259\n",
      "[Episode 10455]  steps:   177 loss: -0.0721498653292656 entropy: 0.5211411714553833\n",
      "[Episode 10456]  steps:    71 loss: -0.10105814039707184 entropy: 0.5731703042984009\n",
      "[Episode 10457]  steps:   103 loss: -0.059668444097042084 entropy: 0.5462737083435059\n",
      "[Episode 10458]  steps:    82 loss: -0.06550679355859756 entropy: 0.5116916298866272\n",
      "[Episode 10459]  steps:   186 loss: -0.022856153547763824 entropy: 0.5126659274101257\n",
      "[Episode 10460]  steps:    88 loss: -0.026433883234858513 entropy: 0.5210526585578918\n",
      "[Episode 10461]  steps:   142 loss: -0.06276030838489532 entropy: 0.5520593523979187\n",
      "[Episode 10462]  steps:    92 loss: -0.09456738084554672 entropy: 0.5501389503479004\n",
      "[Episode 10463]  steps:    89 loss: -0.053007572889328 entropy: 0.5289339423179626\n",
      "[Episode 10464]  steps:    82 loss: -0.11831413209438324 entropy: 0.5237851738929749\n",
      "[Episode 10465]  steps:    80 loss: -0.07965818792581558 entropy: 0.4752618968486786\n",
      "[Episode 10466]  steps:   112 loss: -0.11162716150283813 entropy: 0.6162029504776001\n",
      "[Episode 10467]  steps:   164 loss: -0.11306586116552353 entropy: 0.5363141298294067\n",
      "[Episode 10468]  steps:    90 loss: -0.12928920984268188 entropy: 0.4758457839488983\n",
      "[Episode 10469]  steps:   192 loss: -0.04243346303701401 entropy: 0.5658429861068726\n",
      "[Episode 10470]  steps:    94 loss: -0.08691440522670746 entropy: 0.4724988639354706\n",
      "[Episode 10471]  steps:   129 loss: -0.026718761771917343 entropy: 0.4444330930709839\n",
      "[Episode 10472]  steps:   111 loss: -0.14692933857440948 entropy: 0.52452152967453\n",
      "[Episode 10473]  steps:    90 loss: -0.0938652902841568 entropy: 0.5238928198814392\n",
      "[Episode 10474]  steps:   245 loss: -0.03348281979560852 entropy: 0.49527251720428467\n",
      "[Episode 10475]  steps:   137 loss: -0.13250082731246948 entropy: 0.45523351430892944\n",
      "[Episode 10476]  steps:    79 loss: -0.17151251435279846 entropy: 0.4751051962375641\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for episode in count()\n",
    "    complete = run_episode(policy_net, episode, env,log)\n",
    "\n",
    "    if complete:\n",
    "        print('complete...!')\n",
    "        break\n",
    "    \n",
    "    if episode % 50 == 0:\n",
    "        clear_output(wait=True)\n",
    "        torch.save(policy_net.state_dict(), f'models/{name}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ongoing-david",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
