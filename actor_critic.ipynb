{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "decent-cisco",
   "metadata": {},
   "source": [
    "# Cart-Pole with Actor Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "controversial-response",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "from itertools import count\n",
    "from logger import Logger\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "removable-screw",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyvirtualdisplay.display.Display at 0x7f5c4dff8898>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "virtual_display = Display(visible=0, size=(1400, 900))\n",
    "virtual_display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "executive-washer",
   "metadata": {},
   "outputs": [],
   "source": [
    "############ HYPERPARAMETERS ##############\n",
    "FRAMES = 2\n",
    "RESIZE_PIXELS = 40\n",
    "\n",
    "device = 'cuda:3'\n",
    "\n",
    "POLICY_LR = 1e-6\n",
    "Q_LR = 1e-5\n",
    "GAMMA = 0.99\n",
    "BETA = 0.0\n",
    "END_SCORE = 1000\n",
    "############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bottom-annual",
   "metadata": {},
   "outputs": [],
   "source": [
    "############ ENVIRONMENT ##############\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(\"CartPole-v0\").unwrapped\n",
    "        self.env.reset()\n",
    "        \n",
    "        screen = self.env.render(mode='rgb_array').transpose((2,0,1))\n",
    "        _, self.screen_height, self.screen_width = screen.shape\n",
    "        \n",
    "        self.resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(RESIZE_PIXELS, interpolation=Image.CUBIC),\n",
    "                    T.Grayscale(),\n",
    "                    T.ToTensor()])\n",
    "        \n",
    "        world_width = self.env.x_threshold * 2\n",
    "        self.scale = self.screen_width / world_width\n",
    "        \n",
    "    def get_cart_location(self):\n",
    "        return int(self.env.state[0] * self.scale + self.screen_width / 2.0)\n",
    "        \n",
    "    def get_screen(self):\n",
    "        screen = self.env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "        \n",
    "        screen = screen[:, int(self.screen_height*0.4):int(self.screen_height * 0.8)]\n",
    "        screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "        screen = torch.from_numpy(screen)\n",
    "        return self.resize(screen).unsqueeze(0)\n",
    "    \n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "        \n",
    "    def step(self, action):\n",
    "        return self.env.step(action)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "educated-character",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### NETWORK ##############\n",
    "'''\n",
    "For policy gradient, the network should input the raw pixels, and output the probabilities of choosing either 0 or 1\n",
    "'''\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self,h=60,w=135,device='cuda:0'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        input_channel = 2\n",
    "        hidden_channel = 64\n",
    "        hidden_channel2 = 32\n",
    "        kernel_size = 5\n",
    "        stride = 2\n",
    "        \n",
    "        self.base = nn.Sequential(\n",
    "            nn.Conv2d(input_channel, hidden_channel, kernel_size = kernel_size, stride = stride),\n",
    "            nn.BatchNorm2d(hidden_channel),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_channel, hidden_channel, kernel_size = kernel_size, stride = stride),\n",
    "            nn.BatchNorm2d(hidden_channel),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_channel, hidden_channel2, kernel_size = kernel_size, stride = stride),\n",
    "            nn.BatchNorm2d(hidden_channel2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        def conv2d_size_out(size, kernel_size = kernel_size, stride = stride):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        \n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * hidden_channel2\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(linear_input_size, 24),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(24, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = x.to(self.device)\n",
    "        x = self.base(x)\n",
    "        x = x.flatten(1)\n",
    "        x = self.head(x)\n",
    "        return F.softmax(x,dim = 1)\n",
    "    \n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self,h=60,w=135,device='cuda:0'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        input_channel = 2\n",
    "        hidden_channel = 64\n",
    "        hidden_channel2 = 32\n",
    "        kernel_size = 5\n",
    "        stride = 2\n",
    "        \n",
    "        self.base = nn.Sequential(\n",
    "            nn.Conv2d(input_channel, hidden_channel, kernel_size = kernel_size, stride = stride),\n",
    "            nn.BatchNorm2d(hidden_channel),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_channel, hidden_channel, kernel_size = kernel_size, stride = stride),\n",
    "            nn.BatchNorm2d(hidden_channel),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_channel, hidden_channel2, kernel_size = kernel_size, stride = stride),\n",
    "            nn.BatchNorm2d(hidden_channel2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        def conv2d_size_out(size, kernel_size = kernel_size, stride = stride):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        \n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * hidden_channel2\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(linear_input_size, 24),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(24, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = x.to(self.device)\n",
    "        x = self.base(x)\n",
    "        x = x.flatten(1)\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "unlimited-horror",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "joint-promise",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.step(0)\n",
    "plt.figure()\n",
    "plt.imshow(env.get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy().squeeze(), cmap='gray')\n",
    "plt.title('Example extracted screen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "sunrise-brisbane",
   "metadata": {},
   "outputs": [],
   "source": [
    "screen_shape = env.get_screen().shape\n",
    "policy_net = PolicyNetwork(h = screen_shape[2], w = screen_shape[3], device = device).to(device)\n",
    "\n",
    "q_net = QNetwork(h = screen_shape[2], w = screen_shape[3], device = device).to(device)\n",
    "\n",
    "policy_optimizer = optim.Adam(policy_net.parameters(), lr=POLICY_LR)\n",
    "q_optimizer = optim.Adam(q_net.parameters(), lr = Q_LR)\n",
    "\n",
    "q_mse_loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "constant-party",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Network output: tensor([[0.5685, 0.4315]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "Q Network output: tensor([[-0.1057,  0.1801]], device='cuda:3', grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "p_screen = env.get_screen().to(device)\n",
    "c_screen = env.get_screen().to(device)\n",
    "\n",
    "x = torch.cat((p_screen, c_screen),dim=1)\n",
    "\n",
    "print(\"Policy Network output:\",policy_net(x))\n",
    "print(\"Q Network output:\", q_net(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "recovered-wichita",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(episode, env, logger = None):\n",
    "    policy_net.eval()\n",
    "    q_net.eval()\n",
    "    \n",
    "    state = env.reset()\n",
    "    prev_screen = env.get_screen().to(device)\n",
    "    prev_x = None\n",
    "    prev_y = None\n",
    "    \n",
    "    reward_sum = 0\n",
    "\n",
    "    for steps in count():\n",
    "        screen = env.get_screen().to(device)\n",
    "        \n",
    "        x = torch.cat((prev_screen, screen), dim=1)\n",
    "        \n",
    "        action_prob = policy_net(x)\n",
    "\n",
    "        action = 0 if random.random() < action_prob[0][0] else 1\n",
    "\n",
    "        y = torch.FloatTensor([[1, 0]] if action == 0 else [[0, 1]]).to(device)\n",
    "        \n",
    "        state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        if prev_x is not None:\n",
    "            q_loss = update_q(prev_x, prev_y, reward, x, q_net, policy_net, q_mse_loss, q_optimizer,done)\n",
    "            policy_loss, entropy = update_policy(x, y, q_net, policy_net, policy_optimizer)\n",
    "        \n",
    "        reward_sum += reward\n",
    "        prev_screen = screen\n",
    "        prev_x = x\n",
    "        prev_y = y\n",
    "        \n",
    "        if done or (steps == 1000):\n",
    "            if logger != None:\n",
    "                log.log_scalar(scalar=steps,episode=episode,name='duration')\n",
    "                log.log_scalar(scalar=reward_sum,episode=episode,name='reward')\n",
    "                log.log_scalar(scalar=entropy,episode=episode,name='entropy')\n",
    "                log.log_scalar(scalar=q_loss,episode=episode,name='q_loss')\n",
    "                log.log_scalar(scalar=policy_loss,episode=episode,name='policy_loss')\n",
    "            print(f\"[EPIPSODE] {episode} [STEP] {steps} [Q_LOSS] {q_loss} [POLICY LOSS] {policy_loss}\")\n",
    "            return False\n",
    "                \n",
    "\n",
    "def update_q(prev_state, prev_action, reward, state, q_net, policy_net, loss_function, optimizer,done):\n",
    "    # q_net(s,a) = q_net(s,a) - lr * (reward + gamma * (q_net(s',a) @ policy_net(s')) - q_net(s,a))\n",
    "    \n",
    "    q_net.train()\n",
    "    \n",
    "    q_prev = q_net(prev_state)@prev_action.T\n",
    "    q = q_net(state).detach()\n",
    "    action_prob = policy_net(state).detach()\n",
    "    #action_prob = torch.tensor([[0.5,0.5]],dtype=torch.float).to(device)\n",
    "    \n",
    "    if not done:\n",
    "        loss = loss_function(q_prev,(reward + GAMMA*(q@action_prob.T)))\n",
    "    else:\n",
    "        loss = loss_function(q_prev,torch.tensor(reward,dtype=torch.float).to(device))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "def update_policy(state, action, q_net, policy_net, optimizer):\n",
    "    \n",
    "    # policy_net = policy_net - lr * delta(log(policy_net) * (q_net(s,a) - q_net(s) @ policy_net(s)))\n",
    "    \n",
    "    policy_net.train()\n",
    "    \n",
    "    action_prob = policy_net(state)\n",
    "    q = q_net(state).detach()\n",
    "    \n",
    "    q_policy = q@action_prob.detach().T\n",
    "    q_actual = q@action.T\n",
    "    adv = q_actual - q_policy\n",
    "    \n",
    "    log_action_prob = torch.log(action_prob)\n",
    "    log_lik = action@log_action_prob.T\n",
    "    log_lik_adv = log_lik * adv\n",
    "    \n",
    "    entropy = -log_action_prob@action_prob.T\n",
    "    \n",
    "    loss = -log_lik_adv - BETA*entropy\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item(), entropy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "following-novel",
   "metadata": {},
   "outputs": [],
   "source": [
    "#name = f\"AC hidden=64 lr={LR} max_steps=1000 Adam fullwidth BETA={BETA}\"\n",
    "name = \"AC test\"\n",
    "log = Logger(model_name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honest-heritage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPIPSODE] 1451 [STEP] 25 [Q_LOSS] 0.06580735743045807 [POLICY LOSS] 0.0290910005569458\n",
      "[EPIPSODE] 1452 [STEP] 13 [Q_LOSS] 7.386880874633789 [POLICY LOSS] -0.0013023706851527095\n",
      "[EPIPSODE] 1453 [STEP] 12 [Q_LOSS] 0.3739892840385437 [POLICY LOSS] 0.006106272339820862\n",
      "[EPIPSODE] 1454 [STEP] 8 [Q_LOSS] 0.00031930525437928736 [POLICY LOSS] 0.10075706988573074\n",
      "[EPIPSODE] 1455 [STEP] 38 [Q_LOSS] 5.934669017791748 [POLICY LOSS] -0.013377890922129154\n",
      "[EPIPSODE] 1456 [STEP] 10 [Q_LOSS] 4.834757328033447 [POLICY LOSS] -0.11192527413368225\n",
      "[EPIPSODE] 1457 [STEP] 10 [Q_LOSS] 0.0012291150633245707 [POLICY LOSS] -0.044055696576833725\n",
      "[EPIPSODE] 1458 [STEP] 8 [Q_LOSS] 9.073819160461426 [POLICY LOSS] -0.1927693486213684\n",
      "[EPIPSODE] 1459 [STEP] 10 [Q_LOSS] 7.505154609680176 [POLICY LOSS] 0.10879123210906982\n",
      "[EPIPSODE] 1460 [STEP] 14 [Q_LOSS] 0.07751429826021194 [POLICY LOSS] 0.0742802619934082\n",
      "[EPIPSODE] 1461 [STEP] 21 [Q_LOSS] 2.236389398574829 [POLICY LOSS] -0.009362404234707355\n",
      "[EPIPSODE] 1462 [STEP] 23 [Q_LOSS] 4.237800598144531 [POLICY LOSS] 0.029967384412884712\n",
      "[EPIPSODE] 1463 [STEP] 10 [Q_LOSS] 0.1194223091006279 [POLICY LOSS] -0.05270844325423241\n",
      "[EPIPSODE] 1464 [STEP] 12 [Q_LOSS] 0.013248085044324398 [POLICY LOSS] -0.014345845207571983\n",
      "[EPIPSODE] 1465 [STEP] 27 [Q_LOSS] 0.12663277983665466 [POLICY LOSS] 0.022534817457199097\n",
      "[EPIPSODE] 1466 [STEP] 14 [Q_LOSS] 0.09897472709417343 [POLICY LOSS] -0.0209868885576725\n",
      "[EPIPSODE] 1467 [STEP] 14 [Q_LOSS] 3.545419216156006 [POLICY LOSS] -0.03534157574176788\n",
      "[EPIPSODE] 1468 [STEP] 11 [Q_LOSS] 0.2563471496105194 [POLICY LOSS] 0.11668439209461212\n",
      "[EPIPSODE] 1469 [STEP] 57 [Q_LOSS] 0.2784835398197174 [POLICY LOSS] -0.05179110914468765\n",
      "[EPIPSODE] 1470 [STEP] 8 [Q_LOSS] 0.21201710402965546 [POLICY LOSS] 0.03663749247789383\n",
      "[EPIPSODE] 1471 [STEP] 9 [Q_LOSS] 10.291754722595215 [POLICY LOSS] -0.15104997158050537\n",
      "[EPIPSODE] 1472 [STEP] 14 [Q_LOSS] 23.521411895751953 [POLICY LOSS] -0.05023699998855591\n",
      "[EPIPSODE] 1473 [STEP] 10 [Q_LOSS] 13.741175651550293 [POLICY LOSS] 0.006485214922577143\n",
      "[EPIPSODE] 1474 [STEP] 10 [Q_LOSS] 10.024527549743652 [POLICY LOSS] 0.07917418330907822\n",
      "[EPIPSODE] 1475 [STEP] 16 [Q_LOSS] 0.017846856266260147 [POLICY LOSS] -0.14272013306617737\n",
      "[EPIPSODE] 1476 [STEP] 16 [Q_LOSS] 0.1118142306804657 [POLICY LOSS] -0.061454419046640396\n",
      "[EPIPSODE] 1477 [STEP] 59 [Q_LOSS] 5.924989700317383 [POLICY LOSS] 0.04562598839402199\n",
      "[EPIPSODE] 1478 [STEP] 20 [Q_LOSS] 5.431654453277588 [POLICY LOSS] 0.08090852946043015\n",
      "[EPIPSODE] 1479 [STEP] 11 [Q_LOSS] 0.42262983322143555 [POLICY LOSS] 0.05001499503850937\n",
      "[EPIPSODE] 1480 [STEP] 30 [Q_LOSS] 21.780553817749023 [POLICY LOSS] 0.1286500096321106\n",
      "[EPIPSODE] 1481 [STEP] 22 [Q_LOSS] 0.041160304099321365 [POLICY LOSS] -0.13532216846942902\n",
      "[EPIPSODE] 1482 [STEP] 19 [Q_LOSS] 12.913387298583984 [POLICY LOSS] 0.05813603848218918\n",
      "[EPIPSODE] 1483 [STEP] 13 [Q_LOSS] 0.0021278010681271553 [POLICY LOSS] 0.06033441051840782\n",
      "[EPIPSODE] 1484 [STEP] 30 [Q_LOSS] 31.91520118713379 [POLICY LOSS] -0.0030673115979880095\n",
      "[EPIPSODE] 1485 [STEP] 31 [Q_LOSS] 7.0450310707092285 [POLICY LOSS] -0.041617222130298615\n",
      "[EPIPSODE] 1486 [STEP] 26 [Q_LOSS] 218.53097534179688 [POLICY LOSS] -0.0002998376439791173\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for episode in count():\n",
    "    complete = run_episode(episode, env,log)\n",
    "\n",
    "    if complete:\n",
    "        print('complete...!')\n",
    "        break\n",
    "    \n",
    "    if episode % 50 == 0:\n",
    "        clear_output(wait=True)\n",
    "        torch.save(policy_net.state_dict(), f'models/{name}_policy.pt')\n",
    "        torch.save(q_net.state_dict(), f'models/{name}_q.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solved-ordinance",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
