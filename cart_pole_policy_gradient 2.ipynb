{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fresh-pharmacy",
   "metadata": {},
   "source": [
    "# Cart-Pole with Policy Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "spare-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "from itertools import count\n",
    "from logger import Logger\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "developed-modem",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyvirtualdisplay.display.Display at 0x7f4e44905438>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "virtual_display = Display(visible=0, size=(1400, 900))\n",
    "virtual_display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "thirty-poetry",
   "metadata": {},
   "outputs": [],
   "source": [
    "############ HYPERPARAMETERS ##############\n",
    "FRAMES = 2\n",
    "RESIZE_PIXELS = 60\n",
    "\n",
    "device = 'cuda:0'\n",
    "\n",
    "LR = 1e-5\n",
    "GAMMA = 0.99\n",
    "BETA = -3.0\n",
    "END_SCORE = 1000\n",
    "############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "otherwise-trailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "############ ENVIRONMENT ##############\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(\"CartPole-v0\").unwrapped\n",
    "        self.env.reset()\n",
    "        \n",
    "        screen = self.env.render(mode='rgb_array').transpose((2,0,1))\n",
    "        _, self.screen_height, self.screen_width = screen.shape\n",
    "        \n",
    "        self.resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(RESIZE_PIXELS, interpolation=Image.CUBIC),\n",
    "                    T.Grayscale(),\n",
    "                    T.ToTensor()])\n",
    "        \n",
    "        world_width = self.env.x_threshold * 2\n",
    "        self.scale = self.screen_width / world_width\n",
    "        \n",
    "    def get_cart_location(self):\n",
    "        return int(self.env.state[0] * self.scale + self.screen_width / 2.0)\n",
    "        \n",
    "    def get_screen(self):\n",
    "        screen = self.env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "        \n",
    "        screen = screen[:, int(self.screen_height*0.4):int(self.screen_height * 0.8)]\n",
    "#         view_width = int(self.screen_width * 0.6)\n",
    "#         cart_location = self.get_cart_location()\n",
    "#         if cart_location < view_width // 2:\n",
    "#             slice_range = slice(view_width)\n",
    "#         elif cart_location > (self.screen_width - view_width // 2):\n",
    "#             slice_range = slice(-view_width, None)\n",
    "#         else:\n",
    "#             slice_range = slice(cart_location - view_width // 2,\n",
    "#                                 cart_location + view_width // 2)\n",
    "#         screen = screen[:, :, slice_range]\n",
    "        screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "        screen = torch.from_numpy(screen)\n",
    "        return self.resize(screen).unsqueeze(0)\n",
    "    \n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "        \n",
    "    def step(self, action):\n",
    "        return self.env.step(action)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "distributed-hepatitis",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### NETWORK ##############\n",
    "'''\n",
    "For policy gradient, the network should input the raw pixels, and output the probabilities of choosing either 0 or 1\n",
    "'''\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self,h=60,w=135,device='cuda:0'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        input_channel = 2\n",
    "        hidden_channel = 64\n",
    "        hidden_channel2 = 32\n",
    "        kernel_size = 5\n",
    "        stride = 2\n",
    "        \n",
    "        self.base = nn.Sequential(\n",
    "            nn.Conv2d(input_channel, hidden_channel, kernel_size = kernel_size, stride = stride),\n",
    "            nn.BatchNorm2d(hidden_channel),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_channel, hidden_channel, kernel_size = kernel_size, stride = stride),\n",
    "            nn.BatchNorm2d(hidden_channel),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_channel, hidden_channel2, kernel_size = kernel_size, stride = stride),\n",
    "            nn.BatchNorm2d(hidden_channel2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        def conv2d_size_out(size, kernel_size = kernel_size, stride = stride):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        \n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * hidden_channel2\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(linear_input_size, 24),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(24, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = x.to(self.device)\n",
    "        x = self.base(x)\n",
    "        x = x.flatten(1)\n",
    "        x = self.head(x)\n",
    "        return F.softmax(x,dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fundamental-homework",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "careful-diversity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAACICAYAAAD+r7D/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAT6klEQVR4nO3de5RdZXnH8e/vnLmF3CaBCVASErAgUlsFU2AtUWnxAiiGVqtYy8UbdlUrLK2I0qW0lVbqUtRlvdAiIqCAIhIpCEoJrboEEgwC4RauAXIj5J7MLefpH/udsDOeM3NIZvaZM/P7rHVW9n73Pvt99rt3nnnPu/fZRxGBmZk1n1KjAzAzs93jBG5m1qScwM3MmpQTuJlZk3ICNzNrUk7gZmZNygncCifpTEm/bHQcY4nbxHaHE/g4I+lJSdslbcm9vt7ouBpN0gWSrhzF7S+S9MHR2r5ZNS2NDsBGxckR8YtGB9FMJAlQRFQaHctokNQSEf2NjsNGlnvgE4ikb0q6Ljd/kaTblJkh6UZJayWtT9Ozc+sukvR5Sb9OvfqfStpb0lWSNkm6W9K83Poh6WOSHpf0vKQvSqp6vkk6TNLPJb0g6WFJ7xpiH6ZLulTSSknPppjKktokLZX092m9sqRfSfqspBOAzwDvTrHfm9unCyX9CtgGHCzpfZIelLQ5xf7hQfUvSPVskvSYpBMkXQi8Dvh6/hPPUPuV2m5h2s5dwMuG2OcOSVdKWidpQ2rrfdOymZIuk/RcOm4/SeXHSXpG0qckrQIuk1SSdF6Ke52kayXNzNVzTDq+GyTdK+m4Qcf/X1KbbpZ0q6R9asVsBYkIv8bRC3gSeGONZXsBjwBnkiWc54HZadnewDvSOlOBHwI/yb13EbCcLNFMB5albb2R7JPc94DLcusHcDswEzgwrfvBtOxM4JdpejKwAnhf2s4RKa7Da+zD9cC30/tmAXcBH07LXgmsB14BnA/8BiinZRcAVw7a1iLgaeCPUt2twFvTPgp4A1liPzKtfxSwEXgTWefnAOCw3LY+mNv2kPsFXA1cm9Z7JfDsQJtU2ecPAz9Nx6YMvAaYlpb9N3ANMCPF/4ZUfhzQD1wEtAOTgLNTm8xOZd8GfpDWPwBYB5yU9u1Nab4rt3+PAYembS0CvtDo832ivxoegF8jfECzBL4F2JB7fSi3/GjgBeAp4D1DbOfVwPrc/CLg/Nz8l4Cbc/MnA0tz8wGckJv/O+C2NH0mLybwdwP/N6jubwOfqxLTvkAPMClX9h7g9tz8J4CHyRL5IbnyC6iewP95mPb8CXB2Lq6La6y3iF0TeM39Skm4j5T807J/pXYCfz/wa+BPBpXvD1SAGVXecxzQC3Tkyh4Ejh/0/j6yPzCfAq4YtI1bgDNy+/ePg47nzxp9vk/0l8fAx6dTosYYeETcKelxst7rtQPlkvYCLgZOIOvNAUyVVI6IHWl+dW5T26vMTxlU3Yrc9FPAH1QJaS5wtKQNubIW4Ioa67YCK7MhayDrLebruRy4ELguIh6tso3B8u9F0olkSfbQtO29gPvS4jnATXVscyDWWvvVlaYHt08tV6S6r5bUCVxJ9gljDvBCRKyv8b61EdE9KKbrJeXH+XeQ/WGcC/yVpJNzy1rJPkUNWJWb3sbvH28rmBP4BCPpI2Qfn58DzgX+LS36BPBy4OiIWCXp1cBvyYYSdtcc4IE0fWCqc7AVwB0R8aY6treCrAe+T9S+IPcN4EbgLZKOjYiBW/NqPXZzZ7mkduA64HTghojoS2PKA22wgtpj1YO3X3O/JJXJhjfmAA+l4gNrbJeI6AP+CfindJ3hJrJPGTcBMyV1RsSGOmN6f0T8qkpMK8h64B+qFYeNPb6IOYFIOhT4PPA3wGnAuSlRQzbuvR3YkC5sfW4Eqvxkujg6h2z89Zoq69wIHCrpNEmt6fWnkl4xeMWIWAncCnxJ0rR0Ue5lkt6Q9u80svHhM4GPAZdLGuglrgbm1bqQmrSR/XFbC/Sn3vibc8svBd4n6fhU9wGSDstt/+B69it9ovkxcIGkvSQdDpxRKyhJfybpj1Pi30Q27FFJ7XEz8I3Uzq2SXj/E/n0LuFDS3LTdLkkL0rIrgZMlvUXZBeCOdCF0ds2tWcM5gY9PP9Wu94FfL6mF7D/pRRFxbxpe+AxwRep5foXs4tTzZBe6fjYCcdwALAGWkl1su3TwChGxmSxJnkrWQ1/FixfeqjmdLNEuIxvn/hGwv6QD0z6cHhFbIuL7wGKyYSHILsoCrJN0T7UNp1g+Rja0tB74a2BhbvldZBclLya7mHkH2dADwFeBd6Y7Qb5Wx359lGwIYhXwXeCyGvsLsF/az01k49h38OIQ02lkCf0hYA1wzhDb+Wran1slbSY7zkenfVsBLCA7J9aS9dY/iXPEmKZ0QcJsREkKsouIyxsdi9l45b+uZmZNygnczKxJ7VECT99Ce1jScknnjVRQ1vwiQh4+MRtduz0Gnq6IP0L2ja1ngLvJvhiybOTCMzOzWvakB34UsDwiHo+IXrKvBi8Y5j1mZjZC9uSLPAew6zfJniHdkpQn6SzgLIDJkye/5rDDDhu8ipmZDWHJkiXPR0TX4PJR/yZmRFwCXAIwf/78WLx48WhXaWY2rkiq+qiFPRlCeZbsq8ADZqcyMzMrwJ4k8LuBQyQdJKmN7BtnC4d5j5mZjZDdHkKJiH5JHyV75GQZ+E5EPDDM28zGtIigUqlQKr3Yt8k9+dBsTNmjMfCIuIn6H69pNqZt2bKFrVu3smbNGmbNmkV7ezulUomOjg7K5TLlcrnRIZrtwo+TNUu6u7vZsGEDTz31FBs3bqSlpQVJzJkzh6lTpzJ16tRGh2i2Cydws6S7u5tNmzaxcuVKXnjhBSRRqVSYNGkS5XLZCdzGHD8LxSwplUqUSiVaW1uBF8fDzcYqJ3CznMEXLH0B08YyJ3AzsyblBG5m1qScwM3MmpQTuJlZk3ICNzNrUk7gZmZNygnczKxJOYGbmTUpJ3AzsyblBG5m1qScwM3MmpQTuFlS67knEUFEFByN2fCcwM2Saol6IKn7oVY2Fvl54GbJwC/ySNr5ApgyZQqTJk1qcHRmv889cLNk+/btdHd3A+zsibe0tNDe3k5bW1sjQzOrygncLOnp6aGnp2eXslKpRHt7+84feTAbS5zAzZJSqVR1rNsXMW2sGjaBS5oj6XZJyyQ9IOnsVD5T0s8lPZr+nTH64ZqZ2YB6euD9wCci4nDgGOAjkg4HzgNui4hDgNvSvJmZFWTYBB4RKyPinjS9GXgQOABYAFyeVrscOGWUYjQzsype0hi4pHnAEcCdwL4RsTItWgXsW+M9Z0laLGnx2rVr9yRWMzPLqTuBS5oCXAecExGb8ssiu8JT9SpPRFwSEfMjYn5XV9ceBWtmZi+qK4FLaiVL3ldFxI9T8WpJ+6fl+wNrRidEMzOrpp67UARcCjwYEV/OLVoInJGmzwBuGPnwzMyslnq+Sv9a4DTgPklLU9lngC8A10r6APAU8K5RidDMzKoaNoFHxC+BWk/yOX5kwzEzs3r5m5hmZk3KCdzMrEk5gZuZNSkncDOzJuUEbmbWpPyLPDbhRQR9fX1s27aNnp6enY+ULZVKtLS01HzMrFmjuQduE15E0NvbS29vL319fUgiIiiVSkydOpXW1lZKJf9XsbHHZ6VNeBFBpVLZmcAHlMtlJk+eTLlcdg/cxiQncLMk/0PGZs3ACdwscfK2ZuMEbmbWpJzAzcyalBO4mVmTcgI3M2tSTuBmZk3KCdzMrEk5gZuZNSkncDOzJuUEbmbWpJzAzcyaVN0JXFJZ0m8l3ZjmD5J0p6Tlkq6R1DZ6YZqZ2WAvpQd+NvBgbv4i4OKI+ENgPfCBkQzMzMyGVlcClzQbeCvwX2lewJ8DP0qrXA6cMgrxmRUmIhodgtlLUm8P/CvAuUAlze8NbIiI/jT/DHDAyIZmVoyIoL+///cSeKlU8o852Jg27Jkp6W3AmohYsjsVSDpL0mJJi9euXbs7mzAbVf39/WzatIm+vj4qlcrO8paWFqZPn065XG5gdGa11dO1eC3wdklPAleTDZ18FeiUNPCbmrOBZ6u9OSIuiYj5ETG/q6trBEI2G1n5BD7QC5dEa2sr06ZNcwK3MWvYBB4Rn46I2RExDzgV+J+IeC9wO/DOtNoZwA2jFqWZmf2ePRnc+xTwcUnLycbELx2ZkMzMrB4tw6/yoohYBCxK048DR418SGZmVg9fXjcza1JO4GZQ9VZBSb6AaWPaSxpCMWsmEVHXl3Pyd55Ue3+lUtnl9sJaJPmX7a1QTuA2LlUqFdauXcvWrVuHTb7d3d2sW7eO/v5+SqXSziTc09PDihUrWL9+/bA98Y6ODqZMmUJnZ+dI7YLZsJzAbdyJCLZt28bNN9/MsmXL2L59+5Drt7e309XVxaxZs2htbSUikMSaNWu4++672bZtGzt27Kj5fknMmzePV73qVRx//PEjvTtmNTmB27hTqVTo7e3l1ltv5ZZbbmHDhg1Drt/V1cWxxx7Lcccdx8yZM+nt7aWlpYXVq1ezcOFCnnjiCbq7u2u+XxJHH300lUrFCdwK5QRu49bAM06GG0Lp7++np6eHDRs2EBH09PTQ0dHB1q1b2bFjx7Bj4JLYsWPHkL10s9Hgu1BsXKvnomJE0NfXR39//86E3d/fv/PZKPU+pdAXMK1oTuA24eV76gMJe6BHXc/dJ2aN4iEUm/BKpRIdHR20tbXR1pb9sFRbWxvt7e20tLS4Z21jlhO4jTuSdrkdcDi9vb2sXr2aZcuW0dnZSUdHBzNmzGDLli1VnxNeq04neitaoQk8Iujt7S2ySpuABsaw6x277u3tZdWqVXR3dzN9+nRmzZpFd3c369evp6enp+5hlB07dvj8tkIVmsA3btzIjTfeWGSVNgFVKhW2bt3KihUr6OvrG3b9np4ennvuOVauXLmzJ10qleq+iyUieP7553nggQd8fluhCk3gbW1tzJ07t8gqbQKqVCps3ryZyZMn1/1zaHt6sXJg2MXntxWp0ATe0dHBoYceWmSVNgEN/MLOtGnTCvk9S0l0dHSwzz77+Py2QhWawEulElOnTi2ySpuABsa/i7yDpFwu097e7vPbCuX7wG3cadTdIL4LxYrmBG5m1qScwG1cq/dWwmarywz8RR4bx17qF3r2pJ6BusyK5ARu45IkJk+eTGdn56gnVklMmzaNjo6OUa3HbDAV+bFv/vz5sXjx4sLqs4mrv7+fp59+mo0bN9Lf3z+qdQ38sZgxYwb77bffqNZlE5OkJRExf3C5e+A2LpXLZWbNmkVnZ2chY9MtLS20t7ePej1meU7gNi5JYsqUKY0Ow2xUFTqEImktsBV4vrBK67MPjqkejql+YzEux1SfsRjT3IjoGlxYaAIHkLS42lhOIzmm+jim+o3FuBxTfcZiTLX4viczsyblBG5m1qQakcAvaUCdw3FM9XFM9RuLcTmm+ozFmKoqfAzczMxGhodQzMyaVGEJXNIJkh6WtFzSeUXVOyiGOZJul7RM0gOSzk7lF0h6VtLS9Dqp4LielHRfqntxKpsp6eeSHk3/zig4ppfn2mOppE2Szim6rSR9R9IaSffnyqq2jTJfS+fY7yQdWWBMX5T0UKr3ekmdqXyepO259vpWgTHVPFaSPp3a6WFJbxmNmIaI65pcTE9KWprKR72thsgBDT2ndltEjPoLKAOPAQcDbcC9wOFF1D0ojv2BI9P0VOAR4HDgAuAfio4nF9eTwD6Dyv4dOC9Nnwdc1MD4ysAqYG7RbQW8HjgSuH+4tgFOAm4GBBwD3FlgTG8GWtL0RbmY5uXXK7idqh6rdM7fC7QDB6X/m+Wi4hq0/EvAZ4tqqyFyQEPPqd19FdUDPwpYHhGPR0QvcDWwoKC6d4qIlRFxT5reDDwIHFB0HHVaAFyepi8HTmlcKBwPPBYRTxVdcUT8L/DCoOJabbMA+F5kfgN0Stq/iJgi4taIGHjoym+A2SNd70uNaQgLgKsjoicingCWk/0fLTQuZY+JfBfwg9Gou0Y8tXJAQ8+p3VVUAj8AWJGbf4YGJ05J84AjgDtT0UfTR6TvFD1cAQRwq6Qlks5KZftGxMo0vQrYt+CY8k5l1/9kjWwrqN02Y+U8ez9Zr23AQZJ+K+kOSa8rOJZqx2qstNPrgNUR8WiurLC2GpQDxvo5VdWEvIgpaQpwHXBORGwCvgm8DHg1sJLsY12Rjo2II4ETgY9Ien1+YWSf5Rpyu5CkNuDtwA9TUaPbaheNbJtqJJ0P9ANXpaKVwIERcQTwceD7kqYVFM6YOlZVvIddOwaFtVWVHLDTWDunhlJUAn8WmJObn53KCieplezAXRURPwaIiNURsSMiKsB/MkofJ2uJiGfTv2uA61P9qwc+qqV/1xQZU86JwD0RsTrF2NC2Smq1TUPPM0lnAm8D3puSAGmYYl2aXkI23lzIT9cPcawa/v9RUgvwl8A1A2VFtVW1HMAYPaeGU1QCvxs4RNJBqUd3KrCwoLp3SmNulwIPRsSXc+X5Ma2/AO4f/N5RjGmypKkD02QXw+4na58z0mpnADcUFdMgu/SSGtlWObXaZiFwerpz4BhgY+5j8aiSdAJwLvD2iNiWK++SVE7TBwOHAI8XFFOtY7UQOFVSu6SDUkx3FRFTzhuBhyLimYGCItqqVg5gDJ5TdSnqainZ1dxHyP6qnt+IK7bAsWQfjX4HLE2vk4ArgPtS+UJg/wJjOpjsjoB7gQcG2gbYG7gNeBT4BTCzAe01GVgHTM+VFdpWZH88VgJ9ZOOPH6jVNmR3CvxHOsfuA+YXGNNysrHSgfPqW2ndd6TjuhS4Bzi5wJhqHivg/NRODwMnFnn8Uvl3gb8dtO6ot9UQOaCh59TuvvxNTDOzJjUhL2KamY0HTuBmZk3KCdzMrEk5gZuZNSkncDOzJuUEbmbWpJzAzcyalBO4mVmT+n8FVbTe3V1MqgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.step(0)\n",
    "plt.figure()\n",
    "plt.imshow(env.get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy().squeeze(), cmap='gray')\n",
    "plt.title('Example extracted screen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "decent-italy",
   "metadata": {},
   "outputs": [],
   "source": [
    "screen_shape = env.get_screen().shape\n",
    "policy_net = Network(h = screen_shape[2], w = screen_shape[3], device = device).to(device)\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "banner-indonesia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4471, 0.5529]], device='cuda:0', grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_screen = env.get_screen().to(device)\n",
    "c_screen = env.get_screen().to(device)\n",
    "\n",
    "x = torch.cat((p_screen, c_screen),dim=1)\n",
    "\n",
    "policy_net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "amino-viewer",
   "metadata": {},
   "outputs": [],
   "source": [
    "HISTORY = []\n",
    "\n",
    "def discount_rewards(r):\n",
    "    discounted_r = torch.zeros(r.size())\n",
    "    running_add = 0\n",
    "    for t in reversed(range(len(r))):\n",
    "        running_add = running_add * GAMMA + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "\n",
    "    return discounted_r\n",
    "\n",
    "def run_episode(net, episode, env, logger = None):\n",
    "    net.eval()\n",
    "    state = env.reset()\n",
    "    prev_screen = env.get_screen().to(device)\n",
    "    \n",
    "    reward_sum = 0\n",
    "    xs = torch.FloatTensor([]).to(device)\n",
    "    ys = torch.FloatTensor([]).to(device)\n",
    "    rewards = torch.FloatTensor([]).to(device)\n",
    "    steps = 0\n",
    "\n",
    "    for t in count():\n",
    "        screen = env.get_screen().to(device)\n",
    "        \n",
    "        x = torch.cat((prev_screen, screen), dim=1)\n",
    "        \n",
    "        action_prob = net(x)\n",
    "\n",
    "        action = 0 if random.random() < action_prob[0][0] else 1\n",
    "\n",
    "        y = torch.FloatTensor([[1, 0]] if action == 0 else [[0, 1]]).to(device)\n",
    "        \n",
    "        xs = torch.cat([xs, x])\n",
    "        ys = torch.cat([ys, y])\n",
    "        \n",
    "        prev_screen = screen\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # Reward modification for better stability\n",
    "        x, x_dot, theta, theta_dot = state\n",
    "        r1 = (env.env.x_threshold - abs(x)) / env.env.x_threshold - 0.8\n",
    "        r2 = (env.env.theta_threshold_radians - abs(theta)) / env.env.theta_threshold_radians - 0.5\n",
    "        reward = r1 + r2\n",
    "        #reward = torch.tensor([reward], device=device)\n",
    "        if t >= END_SCORE-1:\n",
    "            reward = reward + 20\n",
    "            done = 1\n",
    "        else: \n",
    "            if done:\n",
    "                reward = reward - 20 \n",
    "        \n",
    "        rewards = torch.cat([rewards, torch.FloatTensor([[reward]]).to(device)])\n",
    "        reward_sum += reward\n",
    "        steps += 1\n",
    "    \n",
    "\n",
    "        if done or steps >= 1000:\n",
    "            adv = discount_rewards(rewards)\n",
    "            #adv = (adv - adv.mean())\n",
    "            #adv = (adv - adv.mean())/(adv.std() + 1e-7)\n",
    "            loss,entropy = learn(xs, ys, adv, net)\n",
    "            print(\"[Episode {:>5}]  steps: {:>5} loss: {:>5} entropy: {:>5}\".format(episode, steps, loss, entropy))\n",
    "            if logger != None:\n",
    "                log.log_scalar(scalar=steps,episode=episode,name='duration')\n",
    "                log.log_scalar(scalar=reward_sum,episode=episode,name='reward')\n",
    "                log.log_scalar(scalar=entropy,episode=episode,name='entropy')\n",
    "                log.log_scalar(scalar=loss,episode=episode,name='loss')\n",
    "            return False\n",
    "\n",
    "def learn(x, y, adv, model):\n",
    "    model.train()\n",
    "    # Loss function, ∑ Ai*logp(yi∣xi), but we need fake lable Y due to autodiff\n",
    "    action_pred = model(x)\n",
    "    log_action_pred = torch.log(action_pred)\n",
    "    \n",
    "    entropy = -torch.sum(action_pred*log_action_pred,dim=1).mean()\n",
    "    \n",
    "    y = Variable(y, requires_grad=True)\n",
    "    adv = Variable(adv).to(device)\n",
    "    # print(action_pred)\n",
    "    log_lik = -y * log_action_pred\n",
    "    # print(y)\n",
    "    log_lik_adv = log_lik * adv\n",
    "    # print(torch.sum(log_lik_adv, 1))\n",
    "    loss = torch.sum(log_lik_adv, 1).mean() - BETA*entropy\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item(), entropy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eleven-promise",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = f\"PG hidden=64 lr={LR} max_steps=1000 Adam fullwidth BETA={BETA}\"\n",
    "#name = \"test\"\n",
    "log = Logger(model_name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternate-beach",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1527, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[Episode 11001]  steps:   108 loss: 2.2425296306610107\n",
      "tensor(0.1640, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[Episode 11002]  steps:   202 loss: 2.853949785232544\n",
      "tensor(0.2032, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[Episode 11003]  steps:    92 loss: 1.8301265239715576\n",
      "tensor(0.1077, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[Episode 11004]  steps:    65 loss: -0.8095523118972778\n",
      "tensor(0.1687, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[Episode 11005]  steps:    84 loss: 0.7292605638504028\n",
      "tensor(0.1361, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[Episode 11006]  steps:    60 loss: -0.51741623878479\n",
      "tensor(0.2016, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[Episode 11007]  steps:   108 loss: 2.477268695831299\n",
      "tensor(0.1765, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[Episode 11008]  steps:   337 loss: 6.087832927703857\n",
      "tensor(0.1977, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[Episode 11009]  steps:   247 loss: 6.240672588348389\n",
      "tensor(0.1968, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[Episode 11010]  steps:    65 loss: 0.365256130695343\n",
      "tensor(0.1628, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[Episode 11011]  steps:   242 loss: 4.4963250160217285\n",
      "tensor(0.2119, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[Episode 11012]  steps:   320 loss: 7.7263665199279785\n",
      "tensor(0.2457, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[Episode 11013]  steps:    91 loss: -0.7045637965202332\n",
      "tensor(0.1989, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[Episode 11014]  steps:   135 loss: 2.5261573791503906\n",
      "tensor(0.1598, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[Episode 11015]  steps:   103 loss: 1.260096788406372\n",
      "tensor(0.1035, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[Episode 11016]  steps:   134 loss: 1.1776589155197144\n",
      "tensor(0.2193, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[Episode 11017]  steps:   190 loss: 4.064984321594238\n",
      "tensor(0.1721, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[Episode 11018]  steps:   177 loss: 2.9507720470428467\n",
      "tensor(0.1930, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[Episode 11019]  steps:   195 loss: 3.7131638526916504\n",
      "tensor(0.0715, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[Episode 11020]  steps:    50 loss: -0.040251851081848145\n",
      "tensor(0.1368, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[Episode 11021]  steps:   136 loss: 0.7611222267150879\n",
      "tensor(0.1472, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[Episode 11022]  steps:   186 loss: 2.8931193351745605\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for episode in range(100000):\n",
    "    complete = run_episode(policy_net, episode, env,log)\n",
    "\n",
    "    if complete:\n",
    "        print('complete...!')\n",
    "        break\n",
    "    \n",
    "    if episode % 50 == 0:\n",
    "        clear_output(wait=True)\n",
    "        torch.save(policy_net.state_dict(), f'models/{name}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loved-disposition",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
