{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sealed-huntington",
   "metadata": {},
   "source": [
    "# Actor Critic with replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "surface-afternoon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "from itertools import count\n",
    "from logger import Logger\n",
    "from pyvirtualdisplay import Display\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "closing-device",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyvirtualdisplay.display.Display at 0x7fe2787c4b70>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "virtual_display = Display(visible=0, size=(1400, 900))\n",
    "virtual_display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "collectible-performance",
   "metadata": {},
   "outputs": [],
   "source": [
    "############ HYPERPARAMETERS ##############\n",
    "FRAMES = 2\n",
    "RESIZE_PIXELS = 40\n",
    "\n",
    "device = 'cuda:3'\n",
    "\n",
    "POLICY_LR = 5e-6\n",
    "Q_LR = 1e-3\n",
    "GAMMA = 0.99\n",
    "BETA = 0.01\n",
    "END_SCORE = 1000\n",
    "MEMORY_SIZE = 150000\n",
    "BATCH_SIZE = 128\n",
    "############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "communist-bride",
   "metadata": {},
   "outputs": [],
   "source": [
    "############ ENVIRONMENT ##############\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(\"CartPole-v0\").unwrapped\n",
    "        self.env.reset()\n",
    "        \n",
    "        screen = self.env.render(mode='rgb_array').transpose((2,0,1))\n",
    "        _, self.screen_height, self.screen_width = screen.shape\n",
    "        \n",
    "        self.resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(RESIZE_PIXELS, interpolation=Image.CUBIC),\n",
    "                    T.Grayscale(),\n",
    "                    T.ToTensor()])\n",
    "        \n",
    "        world_width = self.env.x_threshold * 2\n",
    "        self.scale = self.screen_width / world_width\n",
    "        \n",
    "    def get_cart_location(self):\n",
    "        return int(self.env.state[0] * self.scale + self.screen_width / 2.0)\n",
    "        \n",
    "    def get_screen(self):\n",
    "        screen = self.env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "        \n",
    "        screen = screen[:, int(self.screen_height*0.4):int(self.screen_height * 0.8)]\n",
    "        screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "        screen = torch.from_numpy(screen)\n",
    "        return self.resize(screen).unsqueeze(0)\n",
    "    \n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "        \n",
    "    def step(self, action):\n",
    "        return self.env.step(action)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "sharing-sussex",
   "metadata": {},
   "outputs": [],
   "source": [
    "############ MEMORY ##############\n",
    "\n",
    "Transition = namedtuple('Transition',('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity,memory=None):\n",
    "        self.capacity = capacity\n",
    "        if memory != None:\n",
    "            self.memory = memory\n",
    "        else:\n",
    "            self.memory = []\n",
    "            \n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "weighted-mistress",
   "metadata": {},
   "outputs": [],
   "source": [
    "############ NETWORK ##############\n",
    "'''\n",
    "For policy gradient, the network should input the raw pixels, and output the probabilities of choosing either 0 or 1\n",
    "'''\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self,h=60,w=135,device='cuda:0'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        input_channel = 2\n",
    "        hidden_channel = 32#64\n",
    "        hidden_channel2 = 32\n",
    "        kernel_size = 5\n",
    "        stride = 2\n",
    "        \n",
    "        self.base = nn.Sequential(\n",
    "            nn.Conv2d(input_channel, hidden_channel, kernel_size = kernel_size, stride = stride),\n",
    "            nn.BatchNorm2d(hidden_channel),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_channel, hidden_channel, kernel_size = kernel_size, stride = stride),\n",
    "            nn.BatchNorm2d(hidden_channel),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_channel, hidden_channel2, kernel_size = kernel_size, stride = stride),\n",
    "            nn.BatchNorm2d(hidden_channel2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        def conv2d_size_out(size, kernel_size = kernel_size, stride = stride):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        \n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * hidden_channel2\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(linear_input_size, 24),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(24, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = x.to(self.device)\n",
    "        x = self.base(x)\n",
    "        x = x.flatten(1)\n",
    "        x = self.head(x)\n",
    "        return F.softmax(x,dim = 1)\n",
    "    \n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self,h=60,w=135,device='cuda:0'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        input_channel = 2\n",
    "        hidden_channel = 128#64\n",
    "        hidden_channel2 = 32\n",
    "        kernel_size = 5\n",
    "        stride = 2\n",
    "        \n",
    "        self.base = nn.Sequential(\n",
    "            nn.Conv2d(input_channel, hidden_channel, kernel_size = kernel_size, stride = stride),\n",
    "            nn.BatchNorm2d(hidden_channel),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_channel, hidden_channel, kernel_size = kernel_size, stride = stride),\n",
    "            nn.BatchNorm2d(hidden_channel),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_channel, hidden_channel2, kernel_size = kernel_size, stride = stride),\n",
    "            nn.BatchNorm2d(hidden_channel2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        def conv2d_size_out(size, kernel_size = kernel_size, stride = stride):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        \n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * hidden_channel2\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(linear_input_size, 24),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(24, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = x.to(self.device)\n",
    "        x = self.base(x)\n",
    "        x = x.flatten(1)\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "functional-ivory",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "romantic-focus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.step(0)\n",
    "plt.figure()\n",
    "plt.imshow(env.get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy().squeeze(), cmap='gray')\n",
    "plt.title('Example extracted screen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "portable-being",
   "metadata": {},
   "outputs": [],
   "source": [
    "screen_shape = env.get_screen().shape\n",
    "policy_net = PolicyNetwork(h = screen_shape[2], w = screen_shape[3], device = device).to(device)\n",
    "\n",
    "q_net = QNetwork(h = screen_shape[2], w = screen_shape[3], device = device).to(device)\n",
    "\n",
    "policy_optimizer = optim.Adam(policy_net.parameters(), lr=POLICY_LR, weight_decay=1e-4)\n",
    "q_optimizer = optim.Adam(q_net.parameters(), lr = Q_LR, weight_decay = 1e-4)\n",
    "\n",
    "q_mse_loss = nn.MSELoss()\n",
    "\n",
    "memory = ReplayMemory(MEMORY_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "outer-royal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Network output: tensor([[0.4883, 0.5117]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "Q Network output: tensor([[ 0.1663, -0.3674]], device='cuda:3', grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "p_screen = env.get_screen().to(device)\n",
    "c_screen = env.get_screen().to(device)\n",
    "\n",
    "x = torch.cat((p_screen, c_screen),dim=1)\n",
    "\n",
    "print(\"Policy Network output:\",policy_net(x))\n",
    "print(\"Q Network output:\", q_net(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "informed-proof",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(episode, env, logger = None):\n",
    "    policy_net.eval()\n",
    "    q_net.eval()\n",
    "    \n",
    "    state = env.reset()\n",
    "    prev_screen = env.get_screen().to(device)\n",
    "    prev_x = None\n",
    "    prev_y = None\n",
    "    \n",
    "    reward_sum = 0\n",
    "\n",
    "    for steps in count():\n",
    "        screen = env.get_screen().to(device)\n",
    "        \n",
    "        x = torch.cat((prev_screen, screen), dim=1)\n",
    "        \n",
    "        action_prob = policy_net(x)\n",
    "        action = 0 if random.random() < action_prob[0][0] else 1\n",
    "\n",
    "        y = torch.tensor([[1, 0]] if action == 0 else [[0, 1]],dtype=torch.int64).to(device)\n",
    "        \n",
    "        state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            x = None\n",
    "            reward = -30\n",
    "        \n",
    "        reward = torch.tensor([reward], dtype=float).to(device)\n",
    "        \n",
    "        if prev_x is not None:\n",
    "            memory.push(prev_x, prev_y, x, reward)\n",
    "            policy_loss, entropy = update_policy(prev_x, prev_y, q_net, policy_net, policy_optimizer)\n",
    "            #print(q_net(prev_x).detach(), action_prob)\n",
    "        \n",
    "        reward_sum += reward\n",
    "        prev_screen = screen\n",
    "        prev_x = x\n",
    "        prev_y = y\n",
    "        \n",
    "        if steps % 10 == 0:\n",
    "            q_loss = update_q_batch(memory, q_net, policy_net, q_optimizer)\n",
    "            \n",
    "        if done or (steps == 1000):\n",
    "            if logger != None:\n",
    "                log.log_scalar(scalar=steps,episode=episode,name='duration')\n",
    "                log.log_scalar(scalar=reward_sum,episode=episode,name='reward')\n",
    "                log.log_scalar(scalar=entropy,episode=episode,name='entropy')\n",
    "                log.log_scalar(scalar=q_loss,episode=episode,name='q_loss')\n",
    "                log.log_scalar(scalar=policy_loss,episode=episode,name='policy_loss')\n",
    "            print(f\"[EPIPSODE] {episode} [DURATION] {steps} [Q_LOSS] {q_loss} [POLICY LOSS] {policy_loss}\")\n",
    "            return False, steps\n",
    "                \n",
    "def update_q_batch(memory, q_net, policy_net, optimizer):\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return 0\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    \n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    \n",
    "    state_action_values = torch.sum(q_net(state_batch) * action_batch,dim=1).to(torch.float64)\n",
    "\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    \n",
    "    next_state_values[non_final_mask] = torch.sum(q_net(non_final_next_states).detach() * policy_net(non_final_next_states).detach(),dim=1)\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    loss = F.mse_loss(state_action_values, expected_state_action_values)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "def update_policy(state, action, q_net, policy_net, optimizer):\n",
    "    \n",
    "    # policy_net = policy_net - lr * delta(log(policy_net) * (q_net(s,a) - q_net(s) @ policy_net(s)))\n",
    "    \n",
    "    policy_net.train()\n",
    "    \n",
    "    action_prob = policy_net(state)\n",
    "    q = q_net(state).detach()\n",
    "    \n",
    "    q_policy = q@action_prob.detach().T\n",
    "    q_actual = q@action.float().T\n",
    "    adv = q_actual - q_policy\n",
    "    \n",
    "    log_action_prob = torch.log(action_prob)\n",
    "    log_lik = action.float()@log_action_prob.T\n",
    "    log_lik_adv = log_lik * adv\n",
    "    \n",
    "    entropy = -log_action_prob@action_prob.T\n",
    "    \n",
    "    loss = -log_lik_adv - BETA*entropy\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item(), entropy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "respected-allah",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = f\"AC-RM-2 Q_LR={Q_LR} P_LR={POLICY_LR} BATCH_SIZE={BATCH_SIZE} GAMMA={GAMMA} MEMORY_SIZE={MEMORY_SIZE} BETA={BETA}\"\n",
    "log = Logger(model_name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "congressional-metadata",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPIPSODE] 51 [DURATION] 12 [Q_LOSS] 2.8012318543015624 [POLICY LOSS] -0.20181067287921906\n",
      "[EPIPSODE] 52 [DURATION] 10 [Q_LOSS] 4.4982907588790155 [POLICY LOSS] -0.23760561645030975\n",
      "[EPIPSODE] 53 [DURATION] 10 [Q_LOSS] 6.59480069048692 [POLICY LOSS] -0.03274504095315933\n",
      "[EPIPSODE] 54 [DURATION] 9 [Q_LOSS] 11.05928034506378 [POLICY LOSS] -0.4587441086769104\n",
      "[EPIPSODE] 55 [DURATION] 8 [Q_LOSS] 9.354789871768602 [POLICY LOSS] -0.3421197235584259\n",
      "[EPIPSODE] 56 [DURATION] 16 [Q_LOSS] 12.48574902989975 [POLICY LOSS] -0.13151895999908447\n",
      "[EPIPSODE] 57 [DURATION] 9 [Q_LOSS] 14.502053223895672 [POLICY LOSS] 0.3868798315525055\n",
      "[EPIPSODE] 58 [DURATION] 9 [Q_LOSS] 10.07824777198634 [POLICY LOSS] 0.19673772156238556\n",
      "[EPIPSODE] 59 [DURATION] 11 [Q_LOSS] 11.985130838159677 [POLICY LOSS] 0.0717913955450058\n",
      "[EPIPSODE] 60 [DURATION] 12 [Q_LOSS] 16.329675992813407 [POLICY LOSS] -0.09619217365980148\n",
      "[EPIPSODE] 61 [DURATION] 9 [Q_LOSS] 13.072434014062535 [POLICY LOSS] -0.4763360619544983\n",
      "[EPIPSODE] 62 [DURATION] 9 [Q_LOSS] 4.148105670845286 [POLICY LOSS] -0.6856196522712708\n",
      "[EPIPSODE] 63 [DURATION] 30 [Q_LOSS] 6.3014306140860725 [POLICY LOSS] -0.11786941438913345\n",
      "[EPIPSODE] 64 [DURATION] 13 [Q_LOSS] 5.875017097736638 [POLICY LOSS] 0.42115074396133423\n",
      "[EPIPSODE] 65 [DURATION] 14 [Q_LOSS] 12.666545595717887 [POLICY LOSS] 0.6727126836776733\n",
      "[EPIPSODE] 66 [DURATION] 9 [Q_LOSS] 10.958694340824458 [POLICY LOSS] 0.33939021825790405\n",
      "[EPIPSODE] 67 [DURATION] 13 [Q_LOSS] 10.24674747701102 [POLICY LOSS] -0.40206122398376465\n",
      "[EPIPSODE] 68 [DURATION] 10 [Q_LOSS] 3.255292759111711 [POLICY LOSS] -0.13159504532814026\n",
      "[EPIPSODE] 69 [DURATION] 14 [Q_LOSS] 3.9048053480929 [POLICY LOSS] -0.2710031569004059\n",
      "[EPIPSODE] 70 [DURATION] 11 [Q_LOSS] 5.610023903767363 [POLICY LOSS] -0.1812506765127182\n",
      "[EPIPSODE] 71 [DURATION] 16 [Q_LOSS] 9.37998669456289 [POLICY LOSS] -0.16434067487716675\n",
      "[EPIPSODE] 72 [DURATION] 25 [Q_LOSS] 10.104427709872327 [POLICY LOSS] -0.1670556515455246\n",
      "[EPIPSODE] 73 [DURATION] 11 [Q_LOSS] 10.05352158162821 [POLICY LOSS] -0.14303496479988098\n",
      "[EPIPSODE] 74 [DURATION] 14 [Q_LOSS] 5.879858835454712 [POLICY LOSS] 0.3327842354774475\n",
      "[EPIPSODE] 75 [DURATION] 9 [Q_LOSS] 5.961798328265083 [POLICY LOSS] -0.04435674101114273\n",
      "[EPIPSODE] 76 [DURATION] 10 [Q_LOSS] 5.446481366110582 [POLICY LOSS] -0.08793804794549942\n",
      "[EPIPSODE] 77 [DURATION] 16 [Q_LOSS] 4.726695065355159 [POLICY LOSS] 0.07198251038789749\n",
      "[EPIPSODE] 78 [DURATION] 31 [Q_LOSS] 6.930861781851796 [POLICY LOSS] -0.030868612229824066\n",
      "[EPIPSODE] 79 [DURATION] 8 [Q_LOSS] 8.249336959795667 [POLICY LOSS] -0.04143109545111656\n",
      "[EPIPSODE] 80 [DURATION] 13 [Q_LOSS] 17.569636945515363 [POLICY LOSS] -0.08649719506502151\n",
      "[EPIPSODE] 81 [DURATION] 18 [Q_LOSS] 13.393796865291636 [POLICY LOSS] 0.49768444895744324\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for episode in count():\n",
    "    complete, step = run_episode(episode, env, log)\n",
    "\n",
    "    if complete:\n",
    "        print('complete...!')\n",
    "        break\n",
    "    \n",
    "    if episode % 50 == 0:\n",
    "        clear_output(wait=True)\n",
    "        torch.save(policy_net.state_dict(), f'models/{name}_policy.pt')\n",
    "        torch.save(q_net.state_dict(), f'models/{name}_q.pt')\n",
    "        \n",
    "    if step == 1000:\n",
    "        torch.save(policy_net.state_dict(), f'models/{name}_policy_{episode}.pt')\n",
    "        torch.save(q_net.state_dict(), f'models/{name}_q_{episode}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-profile",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
