{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "minimal-marina",
   "metadata": {},
   "source": [
    "# Actor Critic with replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "handmade-rocket",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "from itertools import count\n",
    "from logger import Logger\n",
    "from pyvirtualdisplay import Display\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "chinese-recruitment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyvirtualdisplay.display.Display at 0x7f7428756b00>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "virtual_display = Display(visible=0, size=(1400, 900))\n",
    "virtual_display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "referenced-lending",
   "metadata": {},
   "outputs": [],
   "source": [
    "############ HYPERPARAMETERS ##############\n",
    "FRAMES = 2\n",
    "RESIZE_PIXELS = 40\n",
    "\n",
    "device = 'cuda:3'\n",
    "\n",
    "POLICY_LR = 5e-6\n",
    "Q_LR = 1e-3\n",
    "GAMMA = 0.99\n",
    "BETA = 0.0\n",
    "END_SCORE = 1000\n",
    "MEMORY_SIZE = 100000\n",
    "BATCH_SIZE = 128\n",
    "############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "hollow-parallel",
   "metadata": {},
   "outputs": [],
   "source": [
    "############ ENVIRONMENT ##############\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(\"CartPole-v0\").unwrapped\n",
    "        self.env.reset()\n",
    "        \n",
    "        screen = self.env.render(mode='rgb_array').transpose((2,0,1))\n",
    "        _, self.screen_height, self.screen_width = screen.shape\n",
    "        \n",
    "        self.resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(RESIZE_PIXELS, interpolation=Image.CUBIC),\n",
    "                    T.Grayscale(),\n",
    "                    T.ToTensor()])\n",
    "        \n",
    "        world_width = self.env.x_threshold * 2\n",
    "        self.scale = self.screen_width / world_width\n",
    "        \n",
    "    def get_cart_location(self):\n",
    "        return int(self.env.state[0] * self.scale + self.screen_width / 2.0)\n",
    "        \n",
    "    def get_screen(self):\n",
    "        screen = self.env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "        \n",
    "        screen = screen[:, int(self.screen_height*0.4):int(self.screen_height * 0.8)]\n",
    "        screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "        screen = torch.from_numpy(screen)\n",
    "        return self.resize(screen).unsqueeze(0)\n",
    "    \n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "        \n",
    "    def step(self, action):\n",
    "        return self.env.step(action)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "waiting-collector",
   "metadata": {},
   "outputs": [],
   "source": [
    "############ MEMORY ##############\n",
    "\n",
    "Transition = namedtuple('Transition',('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity,memory=None):\n",
    "        self.capacity = capacity\n",
    "        if memory != None:\n",
    "            self.memory = memory\n",
    "        else:\n",
    "            self.memory = []\n",
    "            \n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "expected-variety",
   "metadata": {},
   "outputs": [],
   "source": [
    "############ NETWORK ##############\n",
    "'''\n",
    "For policy gradient, the network should input the raw pixels, and output the probabilities of choosing either 0 or 1\n",
    "'''\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self,h=60,w=135,device='cuda:0'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        input_channel = 2\n",
    "        hidden_channel = 64\n",
    "        hidden_channel2 = 32\n",
    "        kernel_size = 5\n",
    "        stride = 2\n",
    "        \n",
    "        self.base = nn.Sequential(\n",
    "            nn.Conv2d(input_channel, hidden_channel, kernel_size = kernel_size, stride = stride),\n",
    "            nn.BatchNorm2d(hidden_channel),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_channel, hidden_channel, kernel_size = kernel_size, stride = stride),\n",
    "            nn.BatchNorm2d(hidden_channel),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_channel, hidden_channel2, kernel_size = kernel_size, stride = stride),\n",
    "            nn.BatchNorm2d(hidden_channel2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        def conv2d_size_out(size, kernel_size = kernel_size, stride = stride):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        \n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * hidden_channel2\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(linear_input_size, 24),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(24, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = x.to(self.device)\n",
    "        x = self.base(x)\n",
    "        x = x.flatten(1)\n",
    "        x = self.head(x)\n",
    "        return F.softmax(x,dim = 1)\n",
    "    \n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self,h=60,w=135,device='cuda:0'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        input_channel = 2\n",
    "        hidden_channel = 64\n",
    "        hidden_channel2 = 32\n",
    "        kernel_size = 5\n",
    "        stride = 2\n",
    "        \n",
    "        self.base = nn.Sequential(\n",
    "            nn.Conv2d(input_channel, hidden_channel, kernel_size = kernel_size, stride = stride),\n",
    "            nn.BatchNorm2d(hidden_channel),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_channel, hidden_channel, kernel_size = kernel_size, stride = stride),\n",
    "            nn.BatchNorm2d(hidden_channel),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_channel, hidden_channel2, kernel_size = kernel_size, stride = stride),\n",
    "            nn.BatchNorm2d(hidden_channel2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        def conv2d_size_out(size, kernel_size = kernel_size, stride = stride):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        \n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * hidden_channel2\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(linear_input_size, 24),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(24, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = x.to(self.device)\n",
    "        x = self.base(x)\n",
    "        x = x.flatten(1)\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "pursuant-actress",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "subtle-vatican",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.step(0)\n",
    "plt.figure()\n",
    "plt.imshow(env.get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy().squeeze(), cmap='gray')\n",
    "plt.title('Example extracted screen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "controlling-compression",
   "metadata": {},
   "outputs": [],
   "source": [
    "screen_shape = env.get_screen().shape\n",
    "policy_net = PolicyNetwork(h = screen_shape[2], w = screen_shape[3], device = device).to(device)\n",
    "\n",
    "q_net = QNetwork(h = screen_shape[2], w = screen_shape[3], device = device).to(device)\n",
    "\n",
    "policy_optimizer = optim.Adam(policy_net.parameters(), lr=POLICY_LR, weight_decay=1e-4)\n",
    "q_optimizer = optim.Adam(q_net.parameters(), lr = Q_LR, weight_decay = 1e-4)\n",
    "\n",
    "q_mse_loss = nn.MSELoss()\n",
    "\n",
    "memory = ReplayMemory(MEMORY_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "excessive-apple",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Network output: tensor([[0.4489, 0.5511]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "Q Network output: tensor([[-0.3404,  0.3986]], device='cuda:3', grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "p_screen = env.get_screen().to(device)\n",
    "c_screen = env.get_screen().to(device)\n",
    "\n",
    "x = torch.cat((p_screen, c_screen),dim=1)\n",
    "\n",
    "print(\"Policy Network output:\",policy_net(x))\n",
    "print(\"Q Network output:\", q_net(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "alpha-choir",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(episode, env, logger = None):\n",
    "    policy_net.eval()\n",
    "    q_net.eval()\n",
    "    \n",
    "    state = env.reset()\n",
    "    prev_screen = env.get_screen().to(device)\n",
    "    prev_x = None\n",
    "    prev_y = None\n",
    "    \n",
    "    reward_sum = 0\n",
    "\n",
    "    for steps in count():\n",
    "        screen = env.get_screen().to(device)\n",
    "        \n",
    "        x = torch.cat((prev_screen, screen), dim=1)\n",
    "        \n",
    "        action_prob = policy_net(x)\n",
    "        action = 0 if random.random() < action_prob[0][0] else 1\n",
    "\n",
    "        y = torch.tensor([[1, 0]] if action == 0 else [[0, 1]],dtype=torch.int64).to(device)\n",
    "        \n",
    "        state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            x = None\n",
    "            reward = -30\n",
    "        \n",
    "        reward = torch.tensor([reward], dtype=float).to(device)\n",
    "        \n",
    "        if prev_x is not None:\n",
    "            memory.push(prev_x, prev_y, x, reward)\n",
    "            policy_loss, entropy = update_policy(prev_x, prev_y, q_net, policy_net, policy_optimizer)\n",
    "            print(q_net(prev_x).detach(), action_prob)\n",
    "        \n",
    "        reward_sum += reward\n",
    "        prev_screen = screen\n",
    "        prev_x = x\n",
    "        prev_y = y\n",
    "        \n",
    "        if steps % 10 == 0:\n",
    "            q_loss = update_q_batch(memory, q_net, policy_net, q_optimizer)\n",
    "            \n",
    "        if done or (steps == 1000):\n",
    "            if logger != None:\n",
    "                log.log_scalar(scalar=steps,episode=episode,name='duration')\n",
    "                log.log_scalar(scalar=reward_sum,episode=episode,name='reward')\n",
    "                log.log_scalar(scalar=entropy,episode=episode,name='entropy')\n",
    "                log.log_scalar(scalar=q_loss,episode=episode,name='q_loss')\n",
    "                log.log_scalar(scalar=policy_loss,episode=episode,name='policy_loss')\n",
    "            print(f\"[EPIPSODE] {episode} [DURATION] {steps} [Q_LOSS] {q_loss} [POLICY LOSS] {policy_loss}\")\n",
    "            return False\n",
    "                \n",
    "def update_q_batch(memory, q_net, policy_net, optimizer):\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return 0\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    \n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    \n",
    "    state_action_values = torch.sum(q_net(state_batch) * action_batch,dim=1).to(torch.float64)\n",
    "\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    \n",
    "    next_state_values[non_final_mask] = torch.sum(q_net(non_final_next_states).detach() * policy_net(non_final_next_states).detach(),dim=1)\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    loss = F.mse_loss(state_action_values, expected_state_action_values)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "def update_policy(state, action, q_net, policy_net, optimizer):\n",
    "    \n",
    "    # policy_net = policy_net - lr * delta(log(policy_net) * (q_net(s,a) - q_net(s) @ policy_net(s)))\n",
    "    \n",
    "    policy_net.train()\n",
    "    \n",
    "    action_prob = policy_net(state)\n",
    "    q = q_net(state).detach()\n",
    "    \n",
    "    q_policy = q@action_prob.detach().T\n",
    "    q_actual = q@action.float().T\n",
    "    adv = q_actual - q_policy\n",
    "    \n",
    "    log_action_prob = torch.log(action_prob)\n",
    "    log_lik = action.float()@log_action_prob.T\n",
    "    log_lik_adv = log_lik * adv\n",
    "    \n",
    "    entropy = -log_action_prob@action_prob.T\n",
    "    \n",
    "    loss = -log_lik_adv - BETA*entropy\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item(), entropy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "round-chester",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = f\"AC-RM Q_LR={Q_LR} P_LR={POLICY_LR} BATCH_SIZE={BATCH_SIZE} GAMMA={GAMMA} MEMORY_SIZE={MEMORY_SIZE}\"\n",
    "log = Logger(model_name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animal-morgan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-13.6298, -15.0216]], device='cuda:3') tensor([[0.4541, 0.5459]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-13.6437, -15.0315]], device='cuda:3') tensor([[0.6331, 0.3669]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-11.1423, -12.2924]], device='cuda:3') tensor([[0.6943, 0.3057]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-14.4943, -15.9105]], device='cuda:3') tensor([[0.4889, 0.5111]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-17.8475, -19.8960]], device='cuda:3') tensor([[0.4566, 0.5434]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-13.5172, -14.8855]], device='cuda:3') tensor([[0.7157, 0.2843]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-11.1707, -12.3455]], device='cuda:3') tensor([[0.7719, 0.2281]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-14.3917, -15.8332]], device='cuda:3') tensor([[0.6151, 0.3849]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-17.6211, -19.6740]], device='cuda:3') tensor([[0.5985, 0.4015]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-13.4619, -14.9226]], device='cuda:3') tensor([[0.6085, 0.3915]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-13.8328, -14.6096]], device='cuda:3') tensor([[0.4190, 0.5810]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-16.2148, -17.5812]], device='cuda:3') tensor([[0.3098, 0.6902]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-11.2173, -11.9946]], device='cuda:3') tensor([[0.3117, 0.6883]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-10.1038, -10.7445]], device='cuda:3') tensor([[0.4856, 0.5144]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-10.9537, -11.5123]], device='cuda:3') tensor([[0.5452, 0.4548]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-11.8877, -12.6502]], device='cuda:3') tensor([[0.7526, 0.2474]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-10.0034, -10.5362]], device='cuda:3') tensor([[0.9230, 0.0770]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-10.2045, -10.7316]], device='cuda:3') tensor([[0.9698, 0.0302]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-11.1974, -11.8884]], device='cuda:3') tensor([[0.9782, 0.0218]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-13.0262, -13.9525]], device='cuda:3') tensor([[0.9815, 0.0185]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-13.8928, -13.4308]], device='cuda:3') tensor([[0.9823, 0.0177]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-14.6812, -14.2974]], device='cuda:3') tensor([[0.9832, 0.0168]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-14.7865, -14.4986]], device='cuda:3') tensor([[0.9825, 0.0175]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-16.5721, -16.4666]], device='cuda:3') tensor([[0.9779, 0.0221]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-13.9454, -13.8378]], device='cuda:3') tensor([[0.9157, 0.0843]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-13.8023, -14.0307]], device='cuda:3') tensor([[0.3745, 0.6255]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-15.0797, -15.6363]], device='cuda:3') tensor([[0.0476, 0.9524]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-13.1949, -13.4155]], device='cuda:3') tensor([[0.0359, 0.9641]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-15.6243, -15.8200]], device='cuda:3') tensor([[0.0338, 0.9662]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-16.7615, -16.6894]], device='cuda:3') tensor([[0.0316, 0.9684]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-14.6537, -13.7296]], device='cuda:3') tensor([[0.0331, 0.9669]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-14.4452, -13.0752]], device='cuda:3') tensor([[0.0343, 0.9657]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-17.4312, -15.4348]], device='cuda:3') tensor([[0.0362, 0.9638]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-16.7448, -14.7882]], device='cuda:3') tensor([[0.0382, 0.9618]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-14.4485, -12.7068]], device='cuda:3') tensor([[0.0387, 0.9613]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-13.4158, -11.7290]], device='cuda:3') tensor([[0.0385, 0.9615]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-12.5489, -10.9173]], device='cuda:3') tensor([[0.0376, 0.9624]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-10.8629,  -9.5121]], device='cuda:3') tensor([[0.0351, 0.9649]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-10.0616,  -9.0978]], device='cuda:3') tensor([[0.0353, 0.9647]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-11.1583, -10.1726]], device='cuda:3') tensor([[0.0399, 0.9601]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-11.3578, -10.2332]], device='cuda:3') tensor([[0.0544, 0.9456]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-16.6006, -16.0291]], device='cuda:3') tensor([[0.1770, 0.8230]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-18.0863, -17.4967]], device='cuda:3') tensor([[0.5822, 0.4178]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-19.4427, -18.9291]], device='cuda:3') tensor([[0.9788, 0.0212]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-14.5112, -14.5244]], device='cuda:3') tensor([[0.9861, 0.0139]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[ -9.9926, -10.2288]], device='cuda:3') tensor([[0.9841, 0.0159]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-20.4748, -23.0693]], device='cuda:3') tensor([[0.9739, 0.0261]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-19.5639, -21.3951]], device='cuda:3') tensor([[0.9524, 0.0476]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "[EPIPSODE] 651 [DURATION] 48 [Q_LOSS] 9.608402818309182 [POLICY LOSS] -6.503140926361084\n",
      "tensor([[-6.3309, -5.9044]], device='cuda:3') tensor([[0.3330, 0.6670]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-6.3309, -5.9044]], device='cuda:3') tensor([[0.2182, 0.7818]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-6.8200, -6.2466]], device='cuda:3') tensor([[0.2287, 0.7713]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-6.1925, -5.7246]], device='cuda:3') tensor([[0.3931, 0.6069]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-7.5861, -7.2070]], device='cuda:3') tensor([[0.5320, 0.4680]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-8.2830, -7.9399]], device='cuda:3') tensor([[0.7191, 0.2809]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-8.1183, -7.8098]], device='cuda:3') tensor([[0.9037, 0.0963]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-11.2624, -11.0971]], device='cuda:3') tensor([[0.9690, 0.0310]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-9.3472, -9.3175]], device='cuda:3') tensor([[0.9803, 0.0197]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-5.1612, -4.9557]], device='cuda:3') tensor([[0.9819, 0.0181]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-3.7735, -3.4393]], device='cuda:3') tensor([[0.9829, 0.0171]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-3.7960, -3.4662]], device='cuda:3') tensor([[0.9835, 0.0165]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-3.7766, -3.4437]], device='cuda:3') tensor([[0.9788, 0.0212]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-4.8135, -4.3584]], device='cuda:3') tensor([[0.9487, 0.0513]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-6.4862, -5.9216]], device='cuda:3') tensor([[0.8841, 0.1159]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-7.4648, -6.9685]], device='cuda:3') tensor([[0.5528, 0.4472]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-10.9422, -10.3712]], device='cuda:3') tensor([[0.2844, 0.7156]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-10.7269,  -9.8653]], device='cuda:3') tensor([[0.1315, 0.8685]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-10.3759,  -9.3458]], device='cuda:3') tensor([[0.0802, 0.9198]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-10.7521,  -9.4996]], device='cuda:3') tensor([[0.0686, 0.9314]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-11.5745,  -9.9720]], device='cuda:3') tensor([[0.0662, 0.9338]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-14.5097, -12.2833]], device='cuda:3') tensor([[0.0776, 0.9224]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-15.9932, -13.4637]], device='cuda:3') tensor([[0.0967, 0.9033]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-14.1372, -12.0705]], device='cuda:3') tensor([[0.0997, 0.9003]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-13.1139, -11.6132]], device='cuda:3') tensor([[0.1094, 0.8906]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-13.8555, -12.3882]], device='cuda:3') tensor([[0.1026, 0.8974]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-14.8846, -12.8032]], device='cuda:3') tensor([[0.1054, 0.8946]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-14.5740, -13.0948]], device='cuda:3') tensor([[0.1051, 0.8949]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-16.2136, -15.5882]], device='cuda:3') tensor([[0.0891, 0.9109]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-15.4279, -13.7983]], device='cuda:3') tensor([[0.0859, 0.9141]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-16.5617, -16.0536]], device='cuda:3') tensor([[0.0693, 0.9307]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-16.3122, -15.6601]], device='cuda:3') tensor([[0.0584, 0.9416]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-16.2808, -15.5411]], device='cuda:3') tensor([[0.0461, 0.9539]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-17.9865, -17.6591]], device='cuda:3') tensor([[0.0403, 0.9597]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-18.4267, -17.6945]], device='cuda:3') tensor([[0.0451, 0.9549]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-22.0870, -20.9480]], device='cuda:3') tensor([[0.1269, 0.8731]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-29.7198, -29.0739]], device='cuda:3') tensor([[0.6084, 0.3916]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "[EPIPSODE] 652 [DURATION] 37 [Q_LOSS] 13.664028107659092 [POLICY LOSS] 0.010866089724004269\n",
      "tensor([[-6.4206, -5.9575]], device='cuda:3') tensor([[0.2622, 0.7378]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-6.5194, -6.0574]], device='cuda:3') tensor([[0.1851, 0.8149]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-7.5376, -6.8291]], device='cuda:3') tensor([[0.1747, 0.8253]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-6.6143, -6.0552]], device='cuda:3') tensor([[0.2377, 0.7623]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-8.1229, -7.5936]], device='cuda:3') tensor([[0.3687, 0.6313]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-8.5679, -8.1385]], device='cuda:3') tensor([[0.6154, 0.3846]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-10.1166,  -9.6500]], device='cuda:3') tensor([[0.8736, 0.1264]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-13.9590, -13.9676]], device='cuda:3') tensor([[0.9598, 0.0402]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-14.8580, -15.0110]], device='cuda:3') tensor([[0.9817, 0.0183]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-12.5429, -12.8841]], device='cuda:3') tensor([[0.9863, 0.0137]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-10.8009, -10.8321]], device='cuda:3') tensor([[0.9865, 0.0135]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-8.2010, -8.1460]], device='cuda:3') tensor([[0.9863, 0.0137]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-6.2702, -6.2166]], device='cuda:3') tensor([[0.9860, 0.0140]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-6.4524, -6.4681]], device='cuda:3') tensor([[0.9854, 0.0146]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-6.7363, -6.8207]], device='cuda:3') tensor([[0.9832, 0.0168]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-7.2502, -7.4474]], device='cuda:3') tensor([[0.9699, 0.0301]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-9.5881, -9.7809]], device='cuda:3') tensor([[0.8497, 0.1503]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-12.1716, -12.2899]], device='cuda:3') tensor([[0.3026, 0.6974]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-15.1363, -15.0598]], device='cuda:3') tensor([[0.0489, 0.9511]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-18.0985, -17.7662]], device='cuda:3') tensor([[0.0386, 0.9614]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-13.5364, -12.2728]], device='cuda:3') tensor([[0.0365, 0.9635]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-10.5111,  -9.1636]], device='cuda:3') tensor([[0.0374, 0.9626]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-8.9685, -7.5115]], device='cuda:3') tensor([[0.0394, 0.9606]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-9.9296, -8.1563]], device='cuda:3') tensor([[0.0454, 0.9546]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-10.8370,  -8.8182]], device='cuda:3') tensor([[0.0566, 0.9434]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-11.2128,  -9.6159]], device='cuda:3') tensor([[0.0626, 0.9374]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-11.3578, -10.7399]], device='cuda:3') tensor([[0.0704, 0.9296]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-10.6997, -10.2058]], device='cuda:3') tensor([[0.0782, 0.9218]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-12.2342, -12.3840]], device='cuda:3') tensor([[0.0729, 0.9271]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "[EPIPSODE] 653 [DURATION] 29 [Q_LOSS] 17.35094415217737 [POLICY LOSS] -0.0009533421834930778\n",
      "tensor([[2.1374, 2.7537]], device='cuda:3') tensor([[0.3687, 0.6313]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[1.9348, 2.5731]], device='cuda:3') tensor([[0.4951, 0.5049]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[1.3858, 1.8422]], device='cuda:3') tensor([[0.4973, 0.5027]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[1.3744, 2.0520]], device='cuda:3') tensor([[0.5196, 0.4804]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[1.3979, 2.0910]], device='cuda:3') tensor([[0.4749, 0.5251]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[1.0052, 1.7208]], device='cuda:3') tensor([[0.4211, 0.5789]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.7529, 1.4260]], device='cuda:3') tensor([[0.1588, 0.8412]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-1.9365, -1.0143]], device='cuda:3') tensor([[0.0792, 0.9208]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-1.0271, -0.2173]], device='cuda:3') tensor([[0.1064, 0.8936]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.9574, 1.6735]], device='cuda:3') tensor([[0.2321, 0.7679]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[2.8343, 3.4432]], device='cuda:3') tensor([[0.3329, 0.6671]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[3.5780, 4.0942]], device='cuda:3') tensor([[0.6889, 0.3111]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[4.3963, 4.8836]], device='cuda:3') tensor([[0.8715, 0.1285]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[3.7001, 4.2374]], device='cuda:3') tensor([[0.8967, 0.1033]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[2.9277, 3.6312]], device='cuda:3') tensor([[0.9593, 0.0407]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[3.3927, 3.9429]], device='cuda:3') tensor([[0.9806, 0.0194]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[3.4263, 3.9476]], device='cuda:3') tensor([[0.9822, 0.0178]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[3.4247, 3.9896]], device='cuda:3') tensor([[0.9652, 0.0348]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[1.9565, 2.8398]], device='cuda:3') tensor([[0.8979, 0.1021]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[1.8258, 2.7122]], device='cuda:3') tensor([[0.6906, 0.3094]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[3.4709, 4.3502]], device='cuda:3') tensor([[0.2151, 0.7849]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[1.4825, 2.5873]], device='cuda:3') tensor([[0.0665, 0.9335]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[1.5817, 2.5659]], device='cuda:3') tensor([[0.0378, 0.9622]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-3.7593, -2.7479]], device='cuda:3') tensor([[0.0335, 0.9665]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-6.5196, -5.1653]], device='cuda:3') tensor([[0.0311, 0.9689]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-5.3313, -3.8544]], device='cuda:3') tensor([[0.0313, 0.9687]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-0.6028,  0.6052]], device='cuda:3') tensor([[0.0321, 0.9679]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3855, 1.6972]], device='cuda:3') tensor([[0.0326, 0.9674]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2716, 1.7721]], device='cuda:3') tensor([[0.0328, 0.9672]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[3.9088, 5.1756]], device='cuda:3') tensor([[0.0325, 0.9675]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[6.6746, 7.4052]], device='cuda:3') tensor([[0.0322, 0.9678]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[7.1150, 7.7731]], device='cuda:3') tensor([[0.0332, 0.9668]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[7.3871, 8.0254]], device='cuda:3') tensor([[0.0355, 0.9645]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[7.4028, 8.2853]], device='cuda:3') tensor([[0.0393, 0.9607]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[6.0742, 6.9492]], device='cuda:3') tensor([[0.0778, 0.9222]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[2.5303, 2.6533]], device='cuda:3') tensor([[0.4677, 0.5323]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-7.1944, -7.4883]], device='cuda:3') tensor([[0.8913, 0.1087]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[2.0047, 1.3122]], device='cuda:3') tensor([[0.9847, 0.0153]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[2.3553, 2.0201]], device='cuda:3') tensor([[0.9881, 0.0119]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[2.5259, 2.3274]], device='cuda:3') tensor([[0.9876, 0.0124]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[4.3590, 4.1291]], device='cuda:3') tensor([[0.9874, 0.0126]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[3.5505, 3.2578]], device='cuda:3') tensor([[0.9871, 0.0129]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[2.8697, 2.4381]], device='cuda:3') tensor([[0.9865, 0.0135]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[4.5264, 4.6779]], device='cuda:3') tensor([[0.9865, 0.0135]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[3.7443, 3.7971]], device='cuda:3') tensor([[0.9862, 0.0138]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[4.0587, 4.2376]], device='cuda:3') tensor([[0.9861, 0.0139]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[4.9658, 5.1772]], device='cuda:3') tensor([[0.9856, 0.0144]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[3.9173, 4.5353]], device='cuda:3') tensor([[0.9856, 0.0144]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[2.0194, 2.8820]], device='cuda:3') tensor([[0.9814, 0.0186]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.8973, 1.8256]], device='cuda:3') tensor([[0.9032, 0.0968]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[1.4063, 2.3130]], device='cuda:3') tensor([[0.2815, 0.7185]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-1.2404, -0.2651]], device='cuda:3') tensor([[0.0390, 0.9610]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-9.5782, -8.1262]], device='cuda:3') tensor([[0.0345, 0.9655]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-13.2717, -11.4566]], device='cuda:3') tensor([[0.0307, 0.9693]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-23.0404, -20.1820]], device='cuda:3') tensor([[0.0316, 0.9684]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-26.1952, -22.4645]], device='cuda:3') tensor([[0.0345, 0.9655]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "[EPIPSODE] 654 [DURATION] 56 [Q_LOSS] 14.572072199913908 [POLICY LOSS] 0.0037514776922762394\n",
      "tensor([[4.2882, 4.2663]], device='cuda:3') tensor([[0.4411, 0.5589]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[5.7581, 5.2921]], device='cuda:3') tensor([[0.4323, 0.5677]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[4.8817, 4.7578]], device='cuda:3') tensor([[0.4627, 0.5373]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[4.7628, 4.6892]], device='cuda:3') tensor([[0.7020, 0.2980]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[5.1716, 4.4790]], device='cuda:3') tensor([[0.7481, 0.2519]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[5.2436, 5.1076]], device='cuda:3') tensor([[0.5427, 0.4573]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[2.8832, 3.2278]], device='cuda:3') tensor([[0.4488, 0.5512]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[4.5169, 4.4658]], device='cuda:3') tensor([[0.7197, 0.2803]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[4.7636, 3.9921]], device='cuda:3') tensor([[0.7857, 0.2143]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[5.6336, 5.5141]], device='cuda:3') tensor([[0.5798, 0.4202]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[4.0296, 4.1566]], device='cuda:3') tensor([[0.2192, 0.7808]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[2.9251, 3.1404]], device='cuda:3') tensor([[0.1665, 0.8335]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[4.5761, 4.3173]], device='cuda:3') tensor([[0.1752, 0.8248]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[5.2700, 5.0232]], device='cuda:3') tensor([[0.3489, 0.6511]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[6.8347, 5.8686]], device='cuda:3') tensor([[0.7337, 0.2663]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[6.5036, 5.2375]], device='cuda:3') tensor([[0.9593, 0.0407]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[5.1030, 3.5264]], device='cuda:3') tensor([[0.9843, 0.0157]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[5.1428, 3.3278]], device='cuda:3') tensor([[0.9868, 0.0132]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[5.4768, 3.5158]], device='cuda:3') tensor([[0.9865, 0.0135]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[8.5409, 7.1826]], device='cuda:3') tensor([[0.9846, 0.0154]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[8.7947, 7.9576]], device='cuda:3') tensor([[0.9756, 0.0244]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[8.2701, 7.7771]], device='cuda:3') tensor([[0.9380, 0.0620]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[8.8617, 8.7154]], device='cuda:3') tensor([[0.6543, 0.3457]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[6.7781, 7.2454]], device='cuda:3') tensor([[0.1229, 0.8771]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[5.5025, 5.9815]], device='cuda:3') tensor([[0.0417, 0.9583]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-0.8838,  0.3021]], device='cuda:3') tensor([[0.0340, 0.9660]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-8.7008, -6.6152]], device='cuda:3') tensor([[0.0320, 0.9680]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[1.4275, 2.6802]], device='cuda:3') tensor([[0.0301, 0.9699]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-6.1829, -4.1601]], device='cuda:3') tensor([[0.0303, 0.9697]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-0.9468,  0.5611]], device='cuda:3') tensor([[0.0313, 0.9687]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[2.7587, 3.7892]], device='cuda:3') tensor([[0.0319, 0.9681]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[3.3920, 4.3090]], device='cuda:3') tensor([[0.0332, 0.9668]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[3.1386, 4.0290]], device='cuda:3') tensor([[0.0359, 0.9641]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[1.8175, 2.3265]], device='cuda:3') tensor([[0.0454, 0.9546]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-3.0217, -3.2512]], device='cuda:3') tensor([[0.0775, 0.9225]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-5.2497, -6.2714]], device='cuda:3') tensor([[0.2489, 0.7511]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-6.5436, -8.3733]], device='cuda:3') tensor([[0.5467, 0.4533]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[ -8.6216, -10.8999]], device='cuda:3') tensor([[0.9210, 0.0790]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-19.5822, -22.5208]], device='cuda:3') tensor([[0.9802, 0.0198]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-14.5594, -17.4723]], device='cuda:3') tensor([[0.9854, 0.0146]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-13.1335, -15.7464]], device='cuda:3') tensor([[0.9872, 0.0128]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-12.5269, -15.1915]], device='cuda:3') tensor([[0.9877, 0.0123]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-12.1275, -14.7022]], device='cuda:3') tensor([[0.9878, 0.0122]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-7.5000, -9.6034]], device='cuda:3') tensor([[0.9878, 0.0122]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-6.8994, -8.9249]], device='cuda:3') tensor([[0.9878, 0.0122]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-4.1143, -5.5768]], device='cuda:3') tensor([[0.9859, 0.0141]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-4.2624, -5.5179]], device='cuda:3') tensor([[0.9784, 0.0216]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-3.7229, -5.1276]], device='cuda:3') tensor([[0.9483, 0.0517]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-7.0298, -8.6103]], device='cuda:3') tensor([[0.7525, 0.2475]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-7.9883, -9.5029]], device='cuda:3') tensor([[0.2541, 0.7459]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-10.8955, -12.3827]], device='cuda:3') tensor([[0.0542, 0.9458]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-14.0407, -14.7617]], device='cuda:3') tensor([[0.0362, 0.9638]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-17.6490, -17.3786]], device='cuda:3') tensor([[0.0354, 0.9646]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-19.8207, -18.1857]], device='cuda:3') tensor([[0.0369, 0.9631]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-19.2614, -17.0625]], device='cuda:3') tensor([[0.0477, 0.9523]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-21.5038, -18.5326]], device='cuda:3') tensor([[0.0608, 0.9392]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-19.6603, -17.2175]], device='cuda:3') tensor([[0.0843, 0.9157]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-18.5056, -16.9306]], device='cuda:3') tensor([[0.1068, 0.8932]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-17.2125, -16.1456]], device='cuda:3') tensor([[0.1204, 0.8796]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-14.1150, -14.7358]], device='cuda:3') tensor([[0.1221, 0.8779]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-11.9328, -12.1037]], device='cuda:3') tensor([[0.1382, 0.8618]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-11.2985, -11.9875]], device='cuda:3') tensor([[0.1529, 0.8471]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-13.7665, -14.7424]], device='cuda:3') tensor([[0.1419, 0.8581]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-15.5423, -16.7226]], device='cuda:3') tensor([[0.1358, 0.8642]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-21.4440, -23.3212]], device='cuda:3') tensor([[0.1284, 0.8716]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-15.2791, -16.9117]], device='cuda:3') tensor([[0.1143, 0.8857]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-14.9587, -16.5796]], device='cuda:3') tensor([[0.1031, 0.8969]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-12.9832, -14.3603]], device='cuda:3') tensor([[0.0972, 0.9028]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-11.4893, -12.4597]], device='cuda:3') tensor([[0.0910, 0.9090]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "[EPIPSODE] 655 [DURATION] 69 [Q_LOSS] 15.811912130770724 [POLICY LOSS] -0.010056514292955399\n",
      "tensor([[-16.7683, -17.5674]], device='cuda:3') tensor([[0.1592, 0.8408]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-16.8924, -17.6972]], device='cuda:3') tensor([[0.3085, 0.6915]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-20.0341, -21.4473]], device='cuda:3') tensor([[0.4679, 0.5321]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-19.4049, -21.0916]], device='cuda:3') tensor([[0.6976, 0.3024]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-19.2358, -21.1594]], device='cuda:3') tensor([[0.9016, 0.0984]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-23.4070, -26.0649]], device='cuda:3') tensor([[0.9726, 0.0274]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-27.9380, -31.7324]], device='cuda:3') tensor([[0.9840, 0.0160]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-17.6931, -20.2655]], device='cuda:3') tensor([[0.9861, 0.0139]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "[EPIPSODE] 656 [DURATION] 8 [Q_LOSS] 22.224800868011712 [POLICY LOSS] 0.0006541920593008399\n",
      "tensor([[6.4622, 4.8497]], device='cuda:3') tensor([[0.8852, 0.1148]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[6.5203, 4.9361]], device='cuda:3') tensor([[0.8720, 0.1280]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[6.2517, 5.4797]], device='cuda:3') tensor([[0.6077, 0.3923]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[3.9559, 3.2763]], device='cuda:3') tensor([[0.2095, 0.7905]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[3.4651, 2.9584]], device='cuda:3') tensor([[0.0550, 0.9450]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-0.8769, -0.4551]], device='cuda:3') tensor([[0.0348, 0.9652]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-3.6230, -2.5969]], device='cuda:3') tensor([[0.0321, 0.9679]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-1.2836, -0.1997]], device='cuda:3') tensor([[0.0313, 0.9687]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-0.2720,  1.0703]], device='cuda:3') tensor([[0.0315, 0.9685]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[3.6322, 4.3114]], device='cuda:3') tensor([[0.0343, 0.9657]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[2.7112, 2.2746]], device='cuda:3') tensor([[0.0377, 0.9623]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[2.5804, 2.0126]], device='cuda:3') tensor([[0.0419, 0.9581]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[3.3116, 2.5889]], device='cuda:3') tensor([[0.0912, 0.9088]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-1.4482, -2.9415]], device='cuda:3') tensor([[0.6392, 0.3608]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-6.1401, -9.0093]], device='cuda:3') tensor([[0.9684, 0.0316]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-4.7682, -7.8250]], device='cuda:3') tensor([[0.9842, 0.0158]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-14.3289, -18.0339]], device='cuda:3') tensor([[0.9851, 0.0149]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-5.5883, -9.2427]], device='cuda:3') tensor([[0.9846, 0.0154]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[ 0.2764, -2.7188]], device='cuda:3') tensor([[0.9836, 0.0164]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[ 1.8538, -0.7366]], device='cuda:3') tensor([[0.9830, 0.0170]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[ 2.2248, -0.1774]], device='cuda:3') tensor([[0.9822, 0.0178]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[6.0936, 4.7487]], device='cuda:3') tensor([[0.9815, 0.0185]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[5.8004, 5.6070]], device='cuda:3') tensor([[0.9829, 0.0171]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[5.2696, 4.7351]], device='cuda:3') tensor([[0.9841, 0.0159]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[4.3632, 3.5290]], device='cuda:3') tensor([[0.9844, 0.0156]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[1.4936, 1.3103]], device='cuda:3') tensor([[0.9813, 0.0187]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-1.4850, -2.1261]], device='cuda:3') tensor([[0.8755, 0.1245]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-4.1234, -4.7441]], device='cuda:3') tensor([[0.1322, 0.8678]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-3.8767, -3.7099]], device='cuda:3') tensor([[0.0371, 0.9629]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-15.7615, -13.9253]], device='cuda:3') tensor([[0.0322, 0.9678]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-28.0890, -24.3699]], device='cuda:3') tensor([[0.0287, 0.9713]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-21.4549, -17.9894]], device='cuda:3') tensor([[0.0313, 0.9687]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-29.9791, -25.1103]], device='cuda:3') tensor([[0.0339, 0.9661]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-25.7366, -21.0450]], device='cuda:3') tensor([[0.0378, 0.9622]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-19.6149, -15.6681]], device='cuda:3') tensor([[0.0412, 0.9588]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-17.3662, -13.7552]], device='cuda:3') tensor([[0.0451, 0.9549]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-15.7255, -12.3357]], device='cuda:3') tensor([[0.0479, 0.9521]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-8.8959, -6.4130]], device='cuda:3') tensor([[0.0463, 0.9537]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-3.7449, -2.0963]], device='cuda:3') tensor([[0.0414, 0.9586]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-6.0662, -3.7629]], device='cuda:3') tensor([[0.0400, 0.9600]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-2.7897, -1.6890]], device='cuda:3') tensor([[0.0355, 0.9645]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-2.1594, -1.0936]], device='cuda:3') tensor([[0.0323, 0.9677]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-1.8541, -0.8324]], device='cuda:3') tensor([[0.0355, 0.9645]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-6.7407, -6.0101]], device='cuda:3') tensor([[0.0465, 0.9535]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-8.4714, -9.0684]], device='cuda:3') tensor([[0.1654, 0.8346]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-15.1124, -16.8800]], device='cuda:3') tensor([[0.5178, 0.4822]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-18.9944, -21.6466]], device='cuda:3') tensor([[0.9714, 0.0286]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-18.3694, -22.1980]], device='cuda:3') tensor([[0.9831, 0.0169]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-21.8848, -26.0283]], device='cuda:3') tensor([[0.9813, 0.0187]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[ -9.8032, -12.7912]], device='cuda:3') tensor([[0.9732, 0.0268]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-10.2098, -12.9900]], device='cuda:3') tensor([[0.9471, 0.0529]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-2.7306, -4.2039]], device='cuda:3') tensor([[0.9217, 0.0783]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-0.1585, -0.7946]], device='cuda:3') tensor([[0.8909, 0.1091]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-0.5777, -1.0595]], device='cuda:3') tensor([[0.8625, 0.1375]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-0.6959, -0.3839]], device='cuda:3') tensor([[0.8579, 0.1421]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-1.4323, -0.8622]], device='cuda:3') tensor([[0.8613, 0.1387]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-0.9210, -0.2690]], device='cuda:3') tensor([[0.8798, 0.1202]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-0.7722,  0.1055]], device='cuda:3') tensor([[0.9156, 0.0844]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-4.2177, -2.9037]], device='cuda:3') tensor([[0.9550, 0.0450]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-7.5378, -5.8244]], device='cuda:3') tensor([[0.9648, 0.0352]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-5.6903, -4.1824]], device='cuda:3') tensor([[0.9653, 0.0347]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-11.7799, -10.0298]], device='cuda:3') tensor([[0.9658, 0.0342]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-19.7403, -17.8667]], device='cuda:3') tensor([[0.9498, 0.0502]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-16.9937, -16.6000]], device='cuda:3') tensor([[0.5360, 0.4640]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-15.2770, -15.8116]], device='cuda:3') tensor([[0.0439, 0.9561]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-24.9886, -25.0617]], device='cuda:3') tensor([[0.0340, 0.9660]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-31.8953, -30.1733]], device='cuda:3') tensor([[0.0325, 0.9675]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-37.8604, -34.3048]], device='cuda:3') tensor([[0.0317, 0.9683]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-40.8965, -36.3248]], device='cuda:3') tensor([[0.0321, 0.9679]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "[EPIPSODE] 657 [DURATION] 69 [Q_LOSS] 15.318151921024084 [POLICY LOSS] 0.0046766712330281734\n",
      "tensor([[-10.4503, -10.6631]], device='cuda:3') tensor([[0.2709, 0.7291]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-11.2894, -11.5044]], device='cuda:3') tensor([[0.2212, 0.7788]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-11.0999, -11.3539]], device='cuda:3') tensor([[0.2301, 0.7699]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-11.2952, -11.5333]], device='cuda:3') tensor([[0.2359, 0.7641]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-10.8803, -11.1418]], device='cuda:3') tensor([[0.2389, 0.7611]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-11.2332, -11.5080]], device='cuda:3') tensor([[0.2423, 0.7577]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-11.7146, -11.9766]], device='cuda:3') tensor([[0.1347, 0.8653]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-11.2273, -10.8343]], device='cuda:3') tensor([[0.1109, 0.8891]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-11.3814, -10.6185]], device='cuda:3') tensor([[0.1179, 0.8821]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-11.4747, -11.0138]], device='cuda:3') tensor([[0.1491, 0.8509]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-13.1306, -13.1178]], device='cuda:3') tensor([[0.2650, 0.7350]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-12.9945, -13.3537]], device='cuda:3') tensor([[0.2654, 0.7346]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-10.2543, -10.4813]], device='cuda:3') tensor([[0.4955, 0.5045]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-13.1997, -14.0438]], device='cuda:3') tensor([[0.6290, 0.3710]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-14.0578, -15.1970]], device='cuda:3') tensor([[0.7640, 0.2360]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-13.5050, -14.8779]], device='cuda:3') tensor([[0.7635, 0.2365]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-11.4913, -12.5548]], device='cuda:3') tensor([[0.8618, 0.1382]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-14.1303, -15.7924]], device='cuda:3') tensor([[0.8584, 0.1416]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-11.4150, -12.7672]], device='cuda:3') tensor([[0.8624, 0.1376]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-11.3230, -12.6538]], device='cuda:3') tensor([[0.8697, 0.1303]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-10.8518, -12.1500]], device='cuda:3') tensor([[0.8117, 0.1883]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-9.0706, -9.9282]], device='cuda:3') tensor([[0.5670, 0.4330]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-10.1850, -10.1061]], device='cuda:3') tensor([[0.5097, 0.4903]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-11.0387, -10.9722]], device='cuda:3') tensor([[0.5033, 0.4967]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-11.9957, -12.2738]], device='cuda:3') tensor([[0.5129, 0.4871]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-12.4083, -12.6579]], device='cuda:3') tensor([[0.4016, 0.5984]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-11.5248, -11.2294]], device='cuda:3') tensor([[0.3324, 0.6676]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-13.6586, -13.4645]], device='cuda:3') tensor([[0.2612, 0.7388]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-12.1440, -11.5076]], device='cuda:3') tensor([[0.1704, 0.8296]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-12.7211, -11.8518]], device='cuda:3') tensor([[0.1658, 0.8342]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-13.5177, -12.7630]], device='cuda:3') tensor([[0.1547, 0.8453]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-13.4885, -12.9492]], device='cuda:3') tensor([[0.1447, 0.8553]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-13.8253, -13.1396]], device='cuda:3') tensor([[0.1900, 0.8100]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-17.6707, -17.3292]], device='cuda:3') tensor([[0.3042, 0.6958]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-17.9522, -17.8143]], device='cuda:3') tensor([[0.5829, 0.4171]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-21.9962, -22.7355]], device='cuda:3') tensor([[0.9030, 0.0970]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-24.9224, -26.5583]], device='cuda:3') tensor([[0.9542, 0.0458]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-20.9107, -22.9372]], device='cuda:3') tensor([[0.9685, 0.0315]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-16.6936, -18.5577]], device='cuda:3') tensor([[0.9742, 0.0258]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-14.9524, -16.9466]], device='cuda:3') tensor([[0.9766, 0.0234]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-13.4131, -15.3416]], device='cuda:3') tensor([[0.9756, 0.0244]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-11.8075, -13.3778]], device='cuda:3') tensor([[0.9589, 0.0411]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-11.2644, -12.4372]], device='cuda:3') tensor([[0.8931, 0.1069]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-12.3002, -13.3451]], device='cuda:3') tensor([[0.7253, 0.2747]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-12.9659, -14.0152]], device='cuda:3') tensor([[0.3098, 0.6902]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-12.5478, -13.4276]], device='cuda:3') tensor([[0.0740, 0.9260]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-12.8245, -13.2118]], device='cuda:3') tensor([[0.0443, 0.9557]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-12.8295, -12.4866]], device='cuda:3') tensor([[0.0412, 0.9588]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-11.0837, -10.3364]], device='cuda:3') tensor([[0.0472, 0.9528]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-12.3760, -11.7318]], device='cuda:3') tensor([[0.0501, 0.9499]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-9.9058, -9.2683]], device='cuda:3') tensor([[0.0559, 0.9441]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "[EPIPSODE] 658 [DURATION] 51 [Q_LOSS] 13.787872017916927 [POLICY LOSS] 0.0016447819070890546\n",
      "tensor([[-12.1894, -11.9777]], device='cuda:3') tensor([[0.3588, 0.6412]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-12.4852, -12.2790]], device='cuda:3') tensor([[0.3510, 0.6490]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-12.3497, -12.1373]], device='cuda:3') tensor([[0.3390, 0.6610]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-12.8079, -12.6106]], device='cuda:3') tensor([[0.4226, 0.5774]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-15.4556, -15.7464]], device='cuda:3') tensor([[0.4402, 0.5598]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-13.8236, -13.9007]], device='cuda:3') tensor([[0.5775, 0.4225]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-15.7890, -16.4134]], device='cuda:3') tensor([[0.5120, 0.4880]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-13.3388, -13.7608]], device='cuda:3') tensor([[0.3552, 0.6448]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-12.0390, -11.9260]], device='cuda:3') tensor([[0.2088, 0.7912]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-12.3769, -12.0508]], device='cuda:3') tensor([[0.1264, 0.8736]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-12.0939, -11.4648]], device='cuda:3') tensor([[0.1021, 0.8979]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-13.8804, -13.3302]], device='cuda:3') tensor([[0.1206, 0.8794]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-17.0916, -16.9046]], device='cuda:3') tensor([[0.0912, 0.9088]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-14.8893, -14.5648]], device='cuda:3') tensor([[0.0812, 0.9188]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-15.2667, -14.9268]], device='cuda:3') tensor([[0.0935, 0.9065]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-18.9332, -18.9505]], device='cuda:3') tensor([[0.1153, 0.8847]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-20.2543, -20.5149]], device='cuda:3') tensor([[0.2544, 0.7456]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-21.8498, -22.8745]], device='cuda:3') tensor([[0.5645, 0.4355]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "[EPIPSODE] 659 [DURATION] 18 [Q_LOSS] 11.25484569105324 [POLICY LOSS] -0.07605141401290894\n",
      "tensor([[-0.7383, -0.6747]], device='cuda:3') tensor([[0.1798, 0.8202]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-0.6994, -0.6276]], device='cuda:3') tensor([[0.1569, 0.8431]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-0.8666, -0.7884]], device='cuda:3') tensor([[0.4351, 0.5649]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-3.8951, -5.0193]], device='cuda:3') tensor([[0.5257, 0.4743]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-1.5416, -2.5191]], device='cuda:3') tensor([[0.8125, 0.1875]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-6.0326, -7.4645]], device='cuda:3') tensor([[0.9613, 0.0387]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-6.4357, -8.1285]], device='cuda:3') tensor([[0.9853, 0.0147]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-6.0059, -8.0787]], device='cuda:3') tensor([[0.9886, 0.0114]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-5.0602, -7.5908]], device='cuda:3') tensor([[0.9892, 0.0108]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-0.8345, -3.1947]], device='cuda:3') tensor([[0.9891, 0.0109]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[ 0.8832, -1.2886]], device='cuda:3') tensor([[0.9889, 0.0111]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[2.5768, 0.9713]], device='cuda:3') tensor([[0.9888, 0.0112]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[2.6281, 1.4858]], device='cuda:3') tensor([[0.9889, 0.0111]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[3.0597, 1.9781]], device='cuda:3') tensor([[0.9885, 0.0115]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[1.5348, 0.5208]], device='cuda:3') tensor([[0.9849, 0.0151]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-0.9615, -1.5328]], device='cuda:3') tensor([[0.9605, 0.0395]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-3.9646, -4.2987]], device='cuda:3') tensor([[0.7977, 0.2023]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-7.9051, -7.6909]], device='cuda:3') tensor([[0.3326, 0.6674]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-15.1839, -13.5630]], device='cuda:3') tensor([[0.1399, 0.8601]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-11.7639,  -9.7291]], device='cuda:3') tensor([[0.0790, 0.9210]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-26.8215, -22.0254]], device='cuda:3') tensor([[0.0699, 0.9301]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-22.1343, -17.3710]], device='cuda:3') tensor([[0.1234, 0.8766]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-22.2879, -17.3734]], device='cuda:3') tensor([[0.2002, 0.7998]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "[EPIPSODE] 660 [DURATION] 23 [Q_LOSS] 13.815929399032825 [POLICY LOSS] 0.06918972730636597\n",
      "tensor([[2.1813, 1.6608]], device='cuda:3') tensor([[0.7376, 0.2624]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[2.2832, 1.7760]], device='cuda:3') tensor([[0.5847, 0.4153]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-1.0133, -0.8415]], device='cuda:3') tensor([[0.5702, 0.4298]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[2.3071, 1.9414]], device='cuda:3') tensor([[0.5249, 0.4751]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[2.0544, 1.6130]], device='cuda:3') tensor([[0.3539, 0.6461]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5357, 0.7982]], device='cuda:3') tensor([[0.1385, 0.8615]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-6.2477, -4.8805]], device='cuda:3') tensor([[0.0982, 0.9018]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-1.3916, -0.2397]], device='cuda:3') tensor([[0.1066, 0.8934]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[2.0115, 2.3193]], device='cuda:3') tensor([[0.1001, 0.8999]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[2.3234, 2.6149]], device='cuda:3') tensor([[0.1116, 0.8884]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[3.0367, 3.4614]], device='cuda:3') tensor([[0.2356, 0.7644]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[2.7306, 2.4714]], device='cuda:3') tensor([[0.5638, 0.4362]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-0.3384, -1.3723]], device='cuda:3') tensor([[0.7879, 0.2121]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-1.5190, -2.6550]], device='cuda:3') tensor([[0.8982, 0.1018]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[2.5502, 1.7389]], device='cuda:3') tensor([[0.9504, 0.0496]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[2.1240, 1.1700]], device='cuda:3') tensor([[0.9748, 0.0252]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[2.6267, 1.4428]], device='cuda:3') tensor([[0.9781, 0.0219]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[2.5062, 1.7236]], device='cuda:3') tensor([[0.9603, 0.0397]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-0.2119, -0.4638]], device='cuda:3') tensor([[0.8920, 0.1080]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-0.6664, -0.7315]], device='cuda:3') tensor([[0.7313, 0.2687]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-0.5454, -0.4671]], device='cuda:3') tensor([[0.3543, 0.6457]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-5.0631, -4.2509]], device='cuda:3') tensor([[0.1063, 0.8937]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-10.4622,  -8.1654]], device='cuda:3') tensor([[0.0646, 0.9354]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-5.4035, -3.2900]], device='cuda:3') tensor([[0.0490, 0.9510]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-1.1692,  0.5821]], device='cuda:3') tensor([[0.0425, 0.9575]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-1.0488,  0.6611]], device='cuda:3') tensor([[0.0442, 0.9558]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[1.0362, 2.1622]], device='cuda:3') tensor([[0.0461, 0.9539]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[1.4606, 2.5506]], device='cuda:3') tensor([[0.0499, 0.9501]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[1.6137, 2.7002]], device='cuda:3') tensor([[0.0602, 0.9398]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.8142, 1.5097]], device='cuda:3') tensor([[0.1134, 0.8866]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-3.2647, -3.1436]], device='cuda:3') tensor([[0.1129, 0.8871]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.4211, 0.9707]], device='cuda:3') tensor([[0.3672, 0.6328]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-3.7531, -4.3350]], device='cuda:3') tensor([[0.7167, 0.2833]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-3.8468, -4.8364]], device='cuda:3') tensor([[0.7738, 0.2262]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-0.4908, -1.3297]], device='cuda:3') tensor([[0.8690, 0.1310]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-0.5970, -1.4904]], device='cuda:3') tensor([[0.9263, 0.0737]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[ 0.2749, -0.6357]], device='cuda:3') tensor([[0.9265, 0.0735]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.9102, 0.4832]], device='cuda:3') tensor([[0.9369, 0.0631]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[ 0.3661, -0.0702]], device='cuda:3') tensor([[0.9342, 0.0658]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-0.0311, -0.4626]], device='cuda:3') tensor([[0.9308, 0.0692]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6531, 0.3227]], device='cuda:3') tensor([[0.8764, 0.1236]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-0.9837, -0.9219]], device='cuda:3') tensor([[0.5794, 0.4206]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-4.9107, -4.1678]], device='cuda:3') tensor([[0.2363, 0.7637]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-4.7340, -3.3808]], device='cuda:3') tensor([[0.1145, 0.8855]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-4.1620, -2.5009]], device='cuda:3') tensor([[0.0635, 0.9365]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-4.9885, -2.8808]], device='cuda:3') tensor([[0.0476, 0.9524]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-2.2681, -0.7175]], device='cuda:3') tensor([[0.0434, 0.9566]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-2.7609, -1.1005]], device='cuda:3') tensor([[0.0444, 0.9556]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-3.6410, -1.5523]], device='cuda:3') tensor([[0.0462, 0.9538]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-2.6075, -0.7265]], device='cuda:3') tensor([[0.0500, 0.9500]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-3.1124, -1.7257]], device='cuda:3') tensor([[0.0621, 0.9379]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-2.8584, -1.7146]], device='cuda:3') tensor([[0.0922, 0.9078]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-2.0899, -1.2150]], device='cuda:3') tensor([[0.1950, 0.8050]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-7.0380, -6.6970]], device='cuda:3') tensor([[0.5505, 0.4495]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-6.2648, -6.6548]], device='cuda:3') tensor([[0.9031, 0.0969]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[ -9.0916, -10.1848]], device='cuda:3') tensor([[0.9796, 0.0204]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-12.8332, -14.8357]], device='cuda:3') tensor([[0.9874, 0.0126]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[ -9.1880, -11.6248]], device='cuda:3') tensor([[0.9889, 0.0111]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-7.3465, -9.6441]], device='cuda:3') tensor([[0.9886, 0.0114]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[ -7.9116, -10.3016]], device='cuda:3') tensor([[0.9880, 0.0120]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "[EPIPSODE] 661 [DURATION] 60 [Q_LOSS] 14.211689730599437 [POLICY LOSS] 0.0003105053910985589\n",
      "tensor([[6.8272, 7.4525]], device='cuda:3') tensor([[0.8321, 0.1679]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[6.8479, 7.4908]], device='cuda:3') tensor([[0.8707, 0.1293]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[3.1017, 4.0895]], device='cuda:3') tensor([[0.5898, 0.4102]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[1.4283, 2.6613]], device='cuda:3') tensor([[0.2637, 0.7363]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[2.6461, 4.2377]], device='cuda:3') tensor([[0.0868, 0.9132]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-0.1015,  2.4564]], device='cuda:3') tensor([[0.0453, 0.9547]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-5.9349, -2.1011]], device='cuda:3') tensor([[0.0357, 0.9643]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-0.9895,  2.6501]], device='cuda:3') tensor([[0.0319, 0.9681]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[4.1118, 7.1946]], device='cuda:3') tensor([[0.0307, 0.9693]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[4.8233, 7.8110]], device='cuda:3') tensor([[0.0299, 0.9701]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[6.5036, 9.5992]], device='cuda:3') tensor([[0.0301, 0.9699]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[ 8.8902, 11.6339]], device='cuda:3') tensor([[0.0303, 0.9697]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[ 9.2565, 11.9584]], device='cuda:3') tensor([[0.0312, 0.9688]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[11.1495, 13.3131]], device='cuda:3') tensor([[0.0341, 0.9659]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[10.1770, 11.9117]], device='cuda:3') tensor([[0.0392, 0.9608]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[10.6489, 12.5280]], device='cuda:3') tensor([[0.1013, 0.8987]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[7.1649, 7.5844]], device='cuda:3') tensor([[0.1628, 0.8372]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[ 9.8373, 10.7705]], device='cuda:3') tensor([[0.6382, 0.3618]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[5.7085, 5.5479]], device='cuda:3') tensor([[0.9768, 0.0232]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[6.1185, 4.9414]], device='cuda:3') tensor([[0.9857, 0.0143]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[5.2689, 3.5211]], device='cuda:3') tensor([[0.9842, 0.0158]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[ 0.2129, -1.9761]], device='cuda:3') tensor([[0.9813, 0.0187]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[2.9576, 1.6730]], device='cuda:3') tensor([[0.9685, 0.0315]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[4.1181, 3.5842]], device='cuda:3') tensor([[0.9509, 0.0491]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[4.6519, 5.0617]], device='cuda:3') tensor([[0.9339, 0.0661]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[4.4717, 4.9429]], device='cuda:3') tensor([[0.9242, 0.0758]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[4.5849, 5.2309]], device='cuda:3') tensor([[0.9252, 0.0748]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[4.7018, 5.3555]], device='cuda:3') tensor([[0.9345, 0.0655]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[5.2294, 5.8445]], device='cuda:3') tensor([[0.9495, 0.0505]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[6.4462, 7.2312]], device='cuda:3') tensor([[0.9686, 0.0314]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[8.2872, 9.7196]], device='cuda:3') tensor([[0.9781, 0.0219]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[ 9.7587, 11.5698]], device='cuda:3') tensor([[0.9812, 0.0188]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[10.7224, 12.7652]], device='cuda:3') tensor([[0.9838, 0.0162]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[ 7.5521, 10.2890]], device='cuda:3') tensor([[0.9749, 0.0251]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6672, 1.8823]], device='cuda:3') tensor([[0.6623, 0.3377]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-7.8709, -6.2376]], device='cuda:3') tensor([[0.0598, 0.9402]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-3.3283,  0.2363]], device='cuda:3') tensor([[0.0380, 0.9620]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-6.4213, -1.3386]], device='cuda:3') tensor([[0.0304, 0.9696]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-4.2116,  1.0363]], device='cuda:3') tensor([[0.0303, 0.9697]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-7.7595, -1.8779]], device='cuda:3') tensor([[0.0336, 0.9664]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-6.8908, -0.7402]], device='cuda:3') tensor([[0.0372, 0.9628]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-10.0045,  -3.6548]], device='cuda:3') tensor([[0.0448, 0.9552]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-14.2553,  -7.3194]], device='cuda:3') tensor([[0.0736, 0.9264]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "[EPIPSODE] 662 [DURATION] 43 [Q_LOSS] 13.107493761395519 [POLICY LOSS] 0.013921783305704594\n",
      "tensor([[11.5224, 12.5122]], device='cuda:3') tensor([[0.9470, 0.0530]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[11.7890, 12.8223]], device='cuda:3') tensor([[0.9632, 0.0368]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[ 9.6576, 10.9749]], device='cuda:3') tensor([[0.9043, 0.0957]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[ 9.6711, 11.2351]], device='cuda:3') tensor([[0.6828, 0.3172]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[ 9.4696, 11.5323]], device='cuda:3') tensor([[0.2016, 0.7984]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[5.3937, 7.9625]], device='cuda:3') tensor([[0.0379, 0.9621]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-5.3586, -1.5829]], device='cuda:3') tensor([[0.0308, 0.9692]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[ 6.6759, 10.3898]], device='cuda:3') tensor([[0.0302, 0.9698]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[11.0496, 14.5088]], device='cuda:3') tensor([[0.0306, 0.9694]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[11.1646, 14.6664]], device='cuda:3') tensor([[0.0304, 0.9696]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[12.3668, 15.8204]], device='cuda:3') tensor([[0.0305, 0.9695]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[14.1749, 17.1351]], device='cuda:3') tensor([[0.0338, 0.9662]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[15.3975, 17.7284]], device='cuda:3') tensor([[0.0365, 0.9635]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[15.9384, 18.0195]], device='cuda:3') tensor([[0.0419, 0.9581]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[16.2323, 18.1034]], device='cuda:3') tensor([[0.0970, 0.9030]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[16.6007, 18.1240]], device='cuda:3') tensor([[0.1514, 0.8486]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[16.6340, 17.9231]], device='cuda:3') tensor([[0.4006, 0.5994]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[16.9192, 18.1383]], device='cuda:3') tensor([[0.8566, 0.1434]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[15.8327, 16.8689]], device='cuda:3') tensor([[0.9754, 0.0246]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[15.2818, 15.8585]], device='cuda:3') tensor([[0.9836, 0.0164]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[14.3822, 14.8414]], device='cuda:3') tensor([[0.9854, 0.0146]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[14.5243, 15.0873]], device='cuda:3') tensor([[0.9849, 0.0151]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[13.3680, 14.2990]], device='cuda:3') tensor([[0.9842, 0.0158]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[13.0678, 14.0092]], device='cuda:3') tensor([[0.9831, 0.0169]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[13.4358, 14.4846]], device='cuda:3') tensor([[0.9767, 0.0233]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[12.2055, 13.4533]], device='cuda:3') tensor([[0.9453, 0.0547]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[ 9.2340, 10.4707]], device='cuda:3') tensor([[0.6382, 0.3618]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[6.1389, 7.7072]], device='cuda:3') tensor([[0.1084, 0.8916]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[5.5059, 7.9060]], device='cuda:3') tensor([[0.0379, 0.9621]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[6.7805, 9.7842]], device='cuda:3') tensor([[0.0313, 0.9687]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[ 7.8228, 10.9412]], device='cuda:3') tensor([[0.0292, 0.9708]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[14.0509, 16.7351]], device='cuda:3') tensor([[0.0289, 0.9711]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[15.4470, 17.5984]], device='cuda:3') tensor([[0.0291, 0.9709]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[15.6579, 17.7260]], device='cuda:3') tensor([[0.0291, 0.9709]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[15.9163, 17.8921]], device='cuda:3') tensor([[0.0337, 0.9663]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[16.5836, 18.1222]], device='cuda:3') tensor([[0.0496, 0.9504]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[16.9644, 18.0876]], device='cuda:3') tensor([[0.0568, 0.9432]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[17.2042, 18.3964]], device='cuda:3') tensor([[0.3491, 0.6509]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[15.7279, 16.5001]], device='cuda:3') tensor([[0.9553, 0.0447]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[14.6507, 14.7520]], device='cuda:3') tensor([[0.9855, 0.0145]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[8.9460, 8.0321]], device='cuda:3') tensor([[0.9858, 0.0142]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[ 1.8842, -0.2748]], device='cuda:3') tensor([[0.9841, 0.0159]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[4.4935, 2.4954]], device='cuda:3') tensor([[0.9777, 0.0223]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[4.3281, 2.6672]], device='cuda:3') tensor([[0.9665, 0.0335]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[6.2677, 5.0380]], device='cuda:3') tensor([[0.9587, 0.0413]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "[EPIPSODE] 663 [DURATION] 45 [Q_LOSS] 22.592968717548516 [POLICY LOSS] 0.0014047976583242416\n",
      "tensor([[7.8915, 8.9978]], device='cuda:3') tensor([[0.0522, 0.9478]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[8.5012, 9.6665]], device='cuda:3') tensor([[0.0401, 0.9599]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[8.4967, 9.6180]], device='cuda:3') tensor([[0.0490, 0.9510]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[11.1536, 11.7520]], device='cuda:3') tensor([[0.1521, 0.8479]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[11.6464, 11.7690]], device='cuda:3') tensor([[0.5014, 0.4986]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[9.0837, 9.3826]], device='cuda:3') tensor([[0.9511, 0.0489]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[8.6063, 7.7525]], device='cuda:3') tensor([[0.9774, 0.0226]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[8.3713, 7.7840]], device='cuda:3') tensor([[0.9808, 0.0192]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[8.5923, 8.2156]], device='cuda:3') tensor([[0.9817, 0.0183]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[8.2438, 7.7360]], device='cuda:3') tensor([[0.9819, 0.0181]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[7.4183, 7.3612]], device='cuda:3') tensor([[0.9823, 0.0177]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[7.9548, 8.0105]], device='cuda:3') tensor([[0.9829, 0.0171]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[9.3354, 9.5478]], device='cuda:3') tensor([[0.9828, 0.0172]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[10.2350, 11.2042]], device='cuda:3') tensor([[0.9810, 0.0190]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[ 9.1156, 10.2436]], device='cuda:3') tensor([[0.9656, 0.0344]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[8.5311, 8.9775]], device='cuda:3') tensor([[0.8231, 0.1769]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[7.4650, 8.3527]], device='cuda:3') tensor([[0.1742, 0.8258]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[4.1209, 5.7063]], device='cuda:3') tensor([[0.0354, 0.9646]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[4.4611, 7.5562]], device='cuda:3') tensor([[0.0303, 0.9697]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[ 9.6371, 12.7440]], device='cuda:3') tensor([[0.0277, 0.9723]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[ 7.9134, 11.4430]], device='cuda:3') tensor([[0.0273, 0.9727]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[5.5744, 9.1965]], device='cuda:3') tensor([[0.0272, 0.9728]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[4.4021, 7.8506]], device='cuda:3') tensor([[0.0272, 0.9728]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[3.3706, 7.0774]], device='cuda:3') tensor([[0.0278, 0.9722]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[3.1171, 6.9615]], device='cuda:3') tensor([[0.0278, 0.9722]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[6.7822, 9.6149]], device='cuda:3') tensor([[0.0283, 0.9717]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[7.3579, 9.9879]], device='cuda:3') tensor([[0.0284, 0.9716]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[ 8.4374, 10.1633]], device='cuda:3') tensor([[0.0288, 0.9712]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[6.9602, 7.9941]], device='cuda:3') tensor([[0.0319, 0.9681]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[5.3354, 6.1305]], device='cuda:3') tensor([[0.0463, 0.9537]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[4.7389, 5.1435]], device='cuda:3') tensor([[0.1857, 0.8143]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.8350, 0.4956]], device='cuda:3') tensor([[0.6098, 0.3902]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[1.8429, 1.5523]], device='cuda:3') tensor([[0.9653, 0.0347]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-8.6454, -9.0625]], device='cuda:3') tensor([[0.9884, 0.0116]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-11.8055, -12.3372]], device='cuda:3') tensor([[0.9893, 0.0107]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-20.4018, -21.4139]], device='cuda:3') tensor([[0.9884, 0.0116]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-24.4391, -26.1746]], device='cuda:3') tensor([[0.9870, 0.0130]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "[EPIPSODE] 664 [DURATION] 37 [Q_LOSS] 18.081960636186718 [POLICY LOSS] 0.00023581601271871477\n",
      "tensor([[5.9326, 6.0867]], device='cuda:3') tensor([[0.7328, 0.2672]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[5.3256, 5.4718]], device='cuda:3') tensor([[0.7853, 0.2147]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[4.4295, 4.5726]], device='cuda:3') tensor([[0.7867, 0.2133]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[4.2411, 4.3500]], device='cuda:3') tensor([[0.7565, 0.2435]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[3.6473, 3.7021]], device='cuda:3') tensor([[0.4741, 0.5259]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-1.7474, -1.5372]], device='cuda:3') tensor([[0.3474, 0.6526]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[2.3510, 2.7219]], device='cuda:3') tensor([[0.1467, 0.8533]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-0.7649, -0.2008]], device='cuda:3') tensor([[0.0740, 0.9260]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2344, 0.9198]], device='cuda:3') tensor([[0.0663, 0.9337]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[3.7997, 4.2383]], device='cuda:3') tensor([[0.0928, 0.9072]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[4.8942, 4.8959]], device='cuda:3') tensor([[0.2765, 0.7235]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[4.4956, 4.5919]], device='cuda:3') tensor([[0.6996, 0.3004]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[5.1631, 5.1714]], device='cuda:3') tensor([[0.9402, 0.0598]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[3.9492, 3.1379]], device='cuda:3') tensor([[0.9673, 0.0327]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[ 0.5468, -0.5200]], device='cuda:3') tensor([[0.9720, 0.0280]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[3.6888, 3.0306]], device='cuda:3') tensor([[0.9728, 0.0272]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[3.6965, 3.8430]], device='cuda:3') tensor([[0.9743, 0.0257]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[4.1933, 4.4335]], device='cuda:3') tensor([[0.9744, 0.0256]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[4.8825, 5.1410]], device='cuda:3') tensor([[0.9700, 0.0300]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[5.0809, 6.0652]], device='cuda:3') tensor([[0.9618, 0.0382]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[7.0310, 7.5477]], device='cuda:3') tensor([[0.9160, 0.0840]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[5.2243, 5.4352]], device='cuda:3') tensor([[0.3679, 0.6321]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-1.3474, -0.8980]], device='cuda:3') tensor([[0.0446, 0.9554]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-1.3334,  0.7059]], device='cuda:3') tensor([[0.0332, 0.9668]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[3.1011, 5.8611]], device='cuda:3') tensor([[0.0305, 0.9695]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[5.6955, 8.5905]], device='cuda:3') tensor([[0.0288, 0.9712]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[ 8.5981, 10.9639]], device='cuda:3') tensor([[0.0284, 0.9716]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-1.4417,  2.6266]], device='cuda:3') tensor([[0.0277, 0.9723]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-5.6063, -0.8238]], device='cuda:3') tensor([[0.0274, 0.9726]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-5.1621, -0.7234]], device='cuda:3') tensor([[0.0275, 0.9725]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-1.5451,  2.0348]], device='cuda:3') tensor([[0.0276, 0.9724]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-0.3077,  3.0717]], device='cuda:3') tensor([[0.0277, 0.9723]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.9614, 4.1174]], device='cuda:3') tensor([[0.0277, 0.9723]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[5.7212, 7.8079]], device='cuda:3') tensor([[0.0283, 0.9717]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[6.4626, 7.3962]], device='cuda:3') tensor([[0.0302, 0.9698]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[6.7466, 7.1146]], device='cuda:3') tensor([[0.0328, 0.9672]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[5.8968, 5.8052]], device='cuda:3') tensor([[0.0419, 0.9581]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[2.2074, 1.9759]], device='cuda:3') tensor([[0.1747, 0.8253]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-5.4787, -6.2253]], device='cuda:3') tensor([[0.8988, 0.1012]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[ -9.2818, -10.3366]], device='cuda:3') tensor([[0.9610, 0.0390]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-19.3558, -20.8469]], device='cuda:3') tensor([[0.9390, 0.0610]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-19.4465, -21.5262]], device='cuda:3') tensor([[0.8544, 0.1456]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-16.6452, -17.7762]], device='cuda:3') tensor([[0.7786, 0.2214]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "[EPIPSODE] 665 [DURATION] 43 [Q_LOSS] 14.101745999141606 [POLICY LOSS] 0.025894515216350555\n",
      "tensor([[-3.0012, -2.9374]], device='cuda:3') tensor([[0.6102, 0.3898]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-2.5618, -2.5591]], device='cuda:3') tensor([[0.7889, 0.2111]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[ 0.6768, -0.0285]], device='cuda:3') tensor([[0.8276, 0.1724]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[ 0.2978, -0.0944]], device='cuda:3') tensor([[0.7030, 0.2970]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-1.9567, -1.7901]], device='cuda:3') tensor([[0.3951, 0.6049]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[1.0244, 1.1431]], device='cuda:3') tensor([[0.1183, 0.8817]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-2.4347, -2.1979]], device='cuda:3') tensor([[0.0597, 0.9403]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-1.7848, -1.2447]], device='cuda:3') tensor([[0.0391, 0.9609]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-8.0893, -6.5891]], device='cuda:3') tensor([[0.0351, 0.9649]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-1.7970, -0.5292]], device='cuda:3') tensor([[0.0341, 0.9659]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[2.5001, 2.9476]], device='cuda:3') tensor([[0.0345, 0.9655]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[2.8534, 3.2128]], device='cuda:3') tensor([[0.0350, 0.9650]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[3.2709, 3.5440]], device='cuda:3') tensor([[0.0379, 0.9621]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[2.6031, 2.4654]], device='cuda:3') tensor([[0.0519, 0.9481]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-0.0179, -0.5168]], device='cuda:3') tensor([[0.1641, 0.8359]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-2.2087, -2.7938]], device='cuda:3') tensor([[0.7824, 0.2176]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-2.7471, -3.6202]], device='cuda:3') tensor([[0.9380, 0.0620]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-7.1984, -8.0438]], device='cuda:3') tensor([[0.9612, 0.0388]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-10.4160, -11.2021]], device='cuda:3') tensor([[0.9660, 0.0340]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-12.1035, -13.0512]], device='cuda:3') tensor([[0.9612, 0.0388]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-9.1416, -9.8126]], device='cuda:3') tensor([[0.9574, 0.0426]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-7.3921, -8.0077]], device='cuda:3') tensor([[0.9557, 0.0443]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-5.3245, -5.7974]], device='cuda:3') tensor([[0.9521, 0.0479]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-1.3503, -0.5848]], device='cuda:3') tensor([[0.9548, 0.0452]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-1.1005,  0.1828]], device='cuda:3') tensor([[0.9575, 0.0425]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-1.2558,  0.2432]], device='cuda:3') tensor([[0.9592, 0.0408]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-0.9779,  0.3942]], device='cuda:3') tensor([[0.9613, 0.0387]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-3.3490, -1.5404]], device='cuda:3') tensor([[0.9449, 0.0551]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-8.1979, -6.7212]], device='cuda:3') tensor([[0.7475, 0.2525]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-1.3926, -1.4724]], device='cuda:3') tensor([[0.0781, 0.9219]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-13.4046, -12.7709]], device='cuda:3') tensor([[0.0319, 0.9681]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-20.6873, -17.0036]], device='cuda:3') tensor([[0.0289, 0.9711]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-23.2957, -17.6286]], device='cuda:3') tensor([[0.0284, 0.9716]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-29.7958, -22.9921]], device='cuda:3') tensor([[0.0281, 0.9719]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-32.2392, -25.0959]], device='cuda:3') tensor([[0.0297, 0.9703]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-36.8923, -28.8655]], device='cuda:3') tensor([[0.0312, 0.9688]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "[EPIPSODE] 666 [DURATION] 36 [Q_LOSS] 19.56454529679555 [POLICY LOSS] 0.0071990154683589935\n",
      "tensor([[1.2867, 1.3127]], device='cuda:3') tensor([[0.1177, 0.8823]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[1.4069, 1.4303]], device='cuda:3') tensor([[0.0566, 0.9434]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[1.7536, 1.2729]], device='cuda:3') tensor([[0.0998, 0.9002]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[2.7050, 2.0626]], device='cuda:3') tensor([[0.3028, 0.6972]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[1.9536, 1.3647]], device='cuda:3') tensor([[0.8905, 0.1095]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-2.5181, -3.7326]], device='cuda:3') tensor([[0.9656, 0.0344]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-13.5378, -14.9611]], device='cuda:3') tensor([[0.9763, 0.0237]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[ -9.7295, -11.2586]], device='cuda:3') tensor([[0.9763, 0.0237]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-6.4232, -7.6563]], device='cuda:3') tensor([[0.9740, 0.0260]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-5.0011, -6.3254]], device='cuda:3') tensor([[0.9730, 0.0270]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-4.4611, -5.9838]], device='cuda:3') tensor([[0.9721, 0.0279]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[4.1333, 3.7233]], device='cuda:3') tensor([[0.9704, 0.0296]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[7.1194, 7.6742]], device='cuda:3') tensor([[0.9740, 0.0260]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[8.2314, 8.6406]], device='cuda:3') tensor([[0.9799, 0.0201]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[7.9324, 7.2926]], device='cuda:3') tensor([[0.9799, 0.0201]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[11.0708, 11.0247]], device='cuda:3') tensor([[0.9818, 0.0182]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[12.4822, 12.8792]], device='cuda:3') tensor([[0.9764, 0.0236]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[13.5318, 13.5188]], device='cuda:3') tensor([[0.9261, 0.0739]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[13.6618, 13.4942]], device='cuda:3') tensor([[0.4480, 0.5520]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[11.8213, 12.1205]], device='cuda:3') tensor([[0.0422, 0.9578]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[6.7859, 7.6400]], device='cuda:3') tensor([[0.0302, 0.9698]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-0.4720,  1.8090]], device='cuda:3') tensor([[0.0281, 0.9719]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-0.6695,  1.8268]], device='cuda:3') tensor([[0.0274, 0.9726]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-3.3741, -0.5443]], device='cuda:3') tensor([[0.0265, 0.9735]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-8.1320, -4.5569]], device='cuda:3') tensor([[0.0265, 0.9735]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-4.9547, -1.8611]], device='cuda:3') tensor([[0.0267, 0.9733]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.8397, 2.8650]], device='cuda:3') tensor([[0.0275, 0.9725]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[1.3402, 3.1875]], device='cuda:3') tensor([[0.0293, 0.9707]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[1.4879, 3.1900]], device='cuda:3') tensor([[0.0316, 0.9684]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-0.1630,  0.7376]], device='cuda:3') tensor([[0.0426, 0.9574]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-10.5957, -10.3963]], device='cuda:3') tensor([[0.0862, 0.9138]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-13.4420, -13.7135]], device='cuda:3') tensor([[0.3538, 0.6462]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-12.2150, -12.8836]], device='cuda:3') tensor([[0.8535, 0.1465]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-22.6854, -23.2825]], device='cuda:3') tensor([[0.9654, 0.0346]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-25.6947, -26.8510]], device='cuda:3') tensor([[0.9856, 0.0144]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-19.4026, -20.6357]], device='cuda:3') tensor([[0.9880, 0.0120]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-15.6019, -16.2137]], device='cuda:3') tensor([[0.9889, 0.0111]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-14.4228, -15.2806]], device='cuda:3') tensor([[0.9894, 0.0106]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-14.7572, -15.6043]], device='cuda:3') tensor([[0.9893, 0.0107]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-16.5758, -17.2279]], device='cuda:3') tensor([[0.9889, 0.0111]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-11.7703, -12.2957]], device='cuda:3') tensor([[0.9887, 0.0113]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-11.8192, -12.3811]], device='cuda:3') tensor([[0.9882, 0.0118]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-12.2778, -12.7503]], device='cuda:3') tensor([[0.9876, 0.0124]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-11.7963, -12.3738]], device='cuda:3') tensor([[0.9864, 0.0136]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-11.2755, -12.1107]], device='cuda:3') tensor([[0.9788, 0.0212]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-12.5200, -13.5017]], device='cuda:3') tensor([[0.8914, 0.1086]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-14.2329, -15.0390]], device='cuda:3') tensor([[0.2651, 0.7349]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-14.4009, -15.1388]], device='cuda:3') tensor([[0.0389, 0.9611]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-14.8279, -14.7284]], device='cuda:3') tensor([[0.0328, 0.9672]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-13.9839, -12.4506]], device='cuda:3') tensor([[0.0342, 0.9658]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-13.1037,  -9.5865]], device='cuda:3') tensor([[0.0366, 0.9634]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-8.8489, -5.8052]], device='cuda:3') tensor([[0.0490, 0.9510]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-10.2122,  -6.6967]], device='cuda:3') tensor([[0.0755, 0.9245]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-9.1940, -6.4946]], device='cuda:3') tensor([[0.1139, 0.8861]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-7.3546, -6.0089]], device='cuda:3') tensor([[0.1458, 0.8542]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-7.2347, -5.7080]], device='cuda:3') tensor([[0.1661, 0.8339]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-11.7669, -10.0875]], device='cuda:3') tensor([[0.1791, 0.8209]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-11.4270,  -9.4101]], device='cuda:3') tensor([[0.1891, 0.8109]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-18.9335, -16.6110]], device='cuda:3') tensor([[0.1830, 0.8170]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "[EPIPSODE] 667 [DURATION] 59 [Q_LOSS] 15.463702183221853 [POLICY LOSS] 0.09173077344894409\n",
      "tensor([[0.1720, 0.1780]], device='cuda:3') tensor([[0.4456, 0.5544]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[1.0678, 1.0118]], device='cuda:3') tensor([[0.3046, 0.6954]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-1.2361, -1.1702]], device='cuda:3') tensor([[0.3008, 0.6992]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-0.5969, -0.5711]], device='cuda:3') tensor([[0.3429, 0.6571]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.8420, 0.8119]], device='cuda:3') tensor([[0.6945, 0.3055]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-3.0170, -3.2968]], device='cuda:3') tensor([[0.9629, 0.0371]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-10.4596, -11.2277]], device='cuda:3') tensor([[0.9739, 0.0261]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[2.0959, 1.6946]], device='cuda:3') tensor([[0.9820, 0.0180]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-2.2514, -2.8759]], device='cuda:3') tensor([[0.9818, 0.0182]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[3.8989, 3.4383]], device='cuda:3') tensor([[0.9741, 0.0259]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[5.5767, 5.4569]], device='cuda:3') tensor([[0.9466, 0.0534]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[5.6998, 5.5298]], device='cuda:3') tensor([[0.8368, 0.1632]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[5.7283, 5.4864]], device='cuda:3') tensor([[0.3874, 0.6126]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[4.8073, 4.7550]], device='cuda:3') tensor([[0.0418, 0.9582]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-2.4155, -1.9278]], device='cuda:3') tensor([[0.0340, 0.9660]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-3.9713, -3.3002]], device='cuda:3') tensor([[0.0320, 0.9680]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-5.3711, -4.8941]], device='cuda:3') tensor([[0.0326, 0.9674]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-6.3497, -5.7395]], device='cuda:3') tensor([[0.0325, 0.9675]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-6.8489, -6.0719]], device='cuda:3') tensor([[0.0344, 0.9656]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-6.1281, -5.7967]], device='cuda:3') tensor([[0.0497, 0.9503]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-5.7769, -6.2466]], device='cuda:3') tensor([[0.0690, 0.9310]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-4.9725, -5.7957]], device='cuda:3') tensor([[0.1065, 0.8935]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-6.9302, -7.7397]], device='cuda:3') tensor([[0.3980, 0.6020]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-12.3132, -13.0022]], device='cuda:3') tensor([[0.7972, 0.2028]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-13.0486, -13.8771]], device='cuda:3') tensor([[0.9546, 0.0454]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-11.3559, -12.4081]], device='cuda:3') tensor([[0.9858, 0.0142]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-10.2215, -10.9647]], device='cuda:3') tensor([[0.9874, 0.0126]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-0.5847, -1.0136]], device='cuda:3') tensor([[0.9889, 0.0111]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-3.7122, -4.5285]], device='cuda:3') tensor([[0.9892, 0.0108]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[ 0.1800, -0.5232]], device='cuda:3') tensor([[0.9879, 0.0121]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[2.9756, 2.0674]], device='cuda:3') tensor([[0.9857, 0.0143]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[2.8834, 1.6687]], device='cuda:3') tensor([[0.9831, 0.0169]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[2.5135, 0.9428]], device='cuda:3') tensor([[0.9589, 0.0411]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[ 1.5746, -0.0748]], device='cuda:3') tensor([[0.7813, 0.2187]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[ 0.1817, -1.6659]], device='cuda:3') tensor([[0.3251, 0.6749]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-5.4860, -6.5692]], device='cuda:3') tensor([[0.1386, 0.8614]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-7.1836, -8.0772]], device='cuda:3') tensor([[0.0556, 0.9444]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-11.1465, -10.2172]], device='cuda:3') tensor([[0.0445, 0.9555]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-11.3503,  -9.7315]], device='cuda:3') tensor([[0.0408, 0.9592]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-11.3813,  -8.9919]], device='cuda:3') tensor([[0.0450, 0.9550]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-9.6556, -7.3052]], device='cuda:3') tensor([[0.0531, 0.9469]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-5.6321, -3.8796]], device='cuda:3') tensor([[0.0612, 0.9388]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-2.8466, -1.8380]], device='cuda:3') tensor([[0.0631, 0.9369]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-3.4589, -2.4324]], device='cuda:3') tensor([[0.0606, 0.9394]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-4.3299, -3.1413]], device='cuda:3') tensor([[0.0540, 0.9460]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-6.4213, -6.0567]], device='cuda:3') tensor([[0.0409, 0.9591]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-11.0668, -11.5158]], device='cuda:3') tensor([[0.0357, 0.9643]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-11.0513, -10.5176]], device='cuda:3') tensor([[0.0326, 0.9674]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-25.0962, -25.7365]], device='cuda:3') tensor([[0.0326, 0.9674]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-20.9316, -21.0230]], device='cuda:3') tensor([[0.0357, 0.9643]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-23.5066, -24.4750]], device='cuda:3') tensor([[0.0582, 0.9418]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "[EPIPSODE] 668 [DURATION] 51 [Q_LOSS] 20.99450431231602 [POLICY LOSS] -0.0012560613686218858\n",
      "tensor([[-1.8564, -3.8833]], device='cuda:3') tensor([[0.4797, 0.5203]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-1.7135, -3.7164]], device='cuda:3') tensor([[0.6958, 0.3042]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-4.9009, -7.0245]], device='cuda:3') tensor([[0.8767, 0.1233]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-3.2541, -5.3595]], device='cuda:3') tensor([[0.9535, 0.0465]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-2.6988, -4.7722]], device='cuda:3') tensor([[0.9786, 0.0214]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-3.3323, -5.5828]], device='cuda:3') tensor([[0.9859, 0.0141]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-2.8419, -5.2206]], device='cuda:3') tensor([[0.9861, 0.0139]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[ 1.2685, -0.6277]], device='cuda:3') tensor([[0.9856, 0.0144]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[ 1.3284, -0.6357]], device='cuda:3') tensor([[0.9809, 0.0191]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[2.4188, 0.6554]], device='cuda:3') tensor([[0.9503, 0.0497]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-1.3141, -3.0488]], device='cuda:3') tensor([[0.6957, 0.3043]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-3.3988, -5.0881]], device='cuda:3') tensor([[0.3285, 0.6715]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-5.2453, -6.8935]], device='cuda:3') tensor([[0.1003, 0.8997]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-12.1594, -12.7562]], device='cuda:3') tensor([[0.0563, 0.9437]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-10.5594, -11.2746]], device='cuda:3') tensor([[0.0377, 0.9623]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-12.0395, -11.9055]], device='cuda:3') tensor([[0.0342, 0.9658]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-8.7179, -8.9191]], device='cuda:3') tensor([[0.0344, 0.9656]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-8.7428, -8.5102]], device='cuda:3') tensor([[0.0359, 0.9641]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-5.8142, -6.0608]], device='cuda:3') tensor([[0.0386, 0.9614]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-5.1835, -5.5300]], device='cuda:3') tensor([[0.0430, 0.9570]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-5.6149, -5.9719]], device='cuda:3') tensor([[0.0557, 0.9443]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-6.6014, -7.6328]], device='cuda:3') tensor([[0.0957, 0.9043]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-11.7753, -13.5615]], device='cuda:3') tensor([[0.2155, 0.7845]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-12.4525, -14.9310]], device='cuda:3') tensor([[0.6208, 0.3792]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-11.9742, -14.6625]], device='cuda:3') tensor([[0.9167, 0.0833]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-18.8778, -21.5787]], device='cuda:3') tensor([[0.9821, 0.0179]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-25.0650, -29.1204]], device='cuda:3') tensor([[0.9878, 0.0122]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-21.2078, -25.5541]], device='cuda:3') tensor([[0.9892, 0.0108]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-25.0224, -28.8998]], device='cuda:3') tensor([[0.9885, 0.0115]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-20.8644, -24.9374]], device='cuda:3') tensor([[0.9877, 0.0123]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "[EPIPSODE] 669 [DURATION] 30 [Q_LOSS] 18.71876421085765 [POLICY LOSS] 0.0005452580517157912\n",
      "tensor([[-6.5676, -8.2042]], device='cuda:3') tensor([[0.3877, 0.6123]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-7.4223, -9.7234]], device='cuda:3') tensor([[0.2468, 0.7532]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-7.1806, -8.0586]], device='cuda:3') tensor([[0.2564, 0.7436]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-6.4017, -7.9300]], device='cuda:3') tensor([[0.3920, 0.6080]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[ -8.7025, -10.7460]], device='cuda:3') tensor([[0.6012, 0.3988]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-4.8332, -7.0238]], device='cuda:3') tensor([[0.8350, 0.1650]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[ -9.7373, -12.0365]], device='cuda:3') tensor([[0.9547, 0.0453]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[ -8.5010, -11.2017]], device='cuda:3') tensor([[0.9763, 0.0237]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-3.3593, -5.9160]], device='cuda:3') tensor([[0.9821, 0.0179]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-4.5593, -7.3171]], device='cuda:3') tensor([[0.9861, 0.0139]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-3.0651, -5.8147]], device='cuda:3') tensor([[0.9864, 0.0136]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-0.7629, -2.7687]], device='cuda:3') tensor([[0.9858, 0.0142]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[ 0.3434, -1.6428]], device='cuda:3') tensor([[0.9829, 0.0171]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-0.9030, -2.3441]], device='cuda:3') tensor([[0.9537, 0.0463]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-5.8166, -6.8535]], device='cuda:3') tensor([[0.7758, 0.2242]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-5.8076, -6.7039]], device='cuda:3') tensor([[0.4625, 0.5375]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-8.7370, -9.6349]], device='cuda:3') tensor([[0.2061, 0.7939]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-18.4286, -17.9509]], device='cuda:3') tensor([[0.0940, 0.9060]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-15.7302, -14.9856]], device='cuda:3') tensor([[0.0505, 0.9495]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-15.6765, -14.2013]], device='cuda:3') tensor([[0.0434, 0.9566]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-5.4411, -4.3291]], device='cuda:3') tensor([[0.0407, 0.9593]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-3.3317, -1.9905]], device='cuda:3') tensor([[0.0382, 0.9618]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.5708, 1.2185]], device='cuda:3') tensor([[0.0393, 0.9607]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[2.2970, 2.8193]], device='cuda:3') tensor([[0.0421, 0.9579]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[3.3185, 3.8194]], device='cuda:3') tensor([[0.0453, 0.9547]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[4.6874, 4.5401]], device='cuda:3') tensor([[0.0640, 0.9360]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[2.5930, 1.6954]], device='cuda:3') tensor([[0.1415, 0.8585]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[4.0411, 2.7561]], device='cuda:3') tensor([[0.4893, 0.5107]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[2.4600, 0.4115]], device='cuda:3') tensor([[0.8525, 0.1475]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[2.4627, 1.0768]], device='cuda:3') tensor([[0.9372, 0.0628]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[6.4478, 4.9716]], device='cuda:3') tensor([[0.9721, 0.0279]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[6.1325, 4.0523]], device='cuda:3') tensor([[0.9859, 0.0141]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[ 2.3636, -0.0814]], device='cuda:3') tensor([[0.9878, 0.0122]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[ 0.5646, -1.2248]], device='cuda:3') tensor([[0.9873, 0.0127]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-0.2816, -1.9536]], device='cuda:3') tensor([[0.9861, 0.0139]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-1.1849, -2.7919]], device='cuda:3') tensor([[0.9834, 0.0166]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-1.2706, -2.6662]], device='cuda:3') tensor([[0.9667, 0.0333]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-4.8258, -5.5761]], device='cuda:3') tensor([[0.8918, 0.1082]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-3.8315, -4.0751]], device='cuda:3') tensor([[0.5276, 0.4724]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-11.4505, -11.2686]], device='cuda:3') tensor([[0.0633, 0.9367]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-18.5204, -17.6451]], device='cuda:3') tensor([[0.0334, 0.9666]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-18.9761, -16.8715]], device='cuda:3') tensor([[0.0301, 0.9699]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-27.0175, -23.6033]], device='cuda:3') tensor([[0.0311, 0.9689]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-26.7183, -22.8984]], device='cuda:3') tensor([[0.0297, 0.9703]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-22.9113, -19.2727]], device='cuda:3') tensor([[0.0294, 0.9706]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-22.6106, -18.9969]], device='cuda:3') tensor([[0.0300, 0.9700]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-16.2074, -13.1935]], device='cuda:3') tensor([[0.0305, 0.9695]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-15.0140, -11.8930]], device='cuda:3') tensor([[0.0302, 0.9698]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-12.9145,  -9.7334]], device='cuda:3') tensor([[0.0301, 0.9699]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-9.8985, -7.0683]], device='cuda:3') tensor([[0.0328, 0.9672]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-6.5810, -3.7501]], device='cuda:3') tensor([[0.0368, 0.9632]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-4.7528, -1.9431]], device='cuda:3') tensor([[0.0400, 0.9600]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-3.1106, -0.3483]], device='cuda:3') tensor([[0.1007, 0.8993]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-3.7139, -0.8020]], device='cuda:3') tensor([[0.8184, 0.1816]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-5.8883, -3.5166]], device='cuda:3') tensor([[0.9806, 0.0194]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-15.3716, -15.5869]], device='cuda:3') tensor([[0.9818, 0.0182]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-19.8356, -22.8188]], device='cuda:3') tensor([[0.9756, 0.0244]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-25.1201, -29.4584]], device='cuda:3') tensor([[0.9640, 0.0360]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-26.5724, -30.8572]], device='cuda:3') tensor([[0.9382, 0.0618]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-18.9622, -20.9733]], device='cuda:3') tensor([[0.9195, 0.0805]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-13.1234, -14.3629]], device='cuda:3') tensor([[0.9088, 0.0912]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-10.6208, -12.1668]], device='cuda:3') tensor([[0.8925, 0.1075]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-10.1365, -11.8481]], device='cuda:3') tensor([[0.8831, 0.1169]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-8.2473, -9.1041]], device='cuda:3') tensor([[0.8849, 0.1151]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-10.0081, -10.1000]], device='cuda:3') tensor([[0.8849, 0.1151]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-10.6045, -10.4741]], device='cuda:3') tensor([[0.8891, 0.1109]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-11.8225, -11.2893]], device='cuda:3') tensor([[0.8964, 0.1036]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-17.0995, -15.9301]], device='cuda:3') tensor([[0.9239, 0.0761]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "[EPIPSODE] 670 [DURATION] 68 [Q_LOSS] 16.64294989054229 [POLICY LOSS] -0.013244802132248878\n",
      "tensor([[6.7933, 7.9293]], device='cuda:3') tensor([[0.9199, 0.0801]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[7.1157, 8.2541]], device='cuda:3') tensor([[0.9631, 0.0369]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[4.4238, 5.9202]], device='cuda:3') tensor([[0.9278, 0.0722]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[5.2867, 6.9847]], device='cuda:3') tensor([[0.7084, 0.2916]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-0.4957,  1.4486]], device='cuda:3') tensor([[0.5725, 0.4275]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[6.6988, 8.5903]], device='cuda:3') tensor([[0.7283, 0.2717]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[12.1105, 13.3511]], device='cuda:3') tensor([[0.4807, 0.5193]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[6.3369, 8.2514]], device='cuda:3') tensor([[0.2437, 0.7563]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.6616, 3.2145]], device='cuda:3') tensor([[0.1476, 0.8524]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-1.9699,  0.9966]], device='cuda:3') tensor([[0.0607, 0.9393]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-10.0983,  -5.4415]], device='cuda:3') tensor([[0.0349, 0.9651]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-3.8610,  0.2474]], device='cuda:3') tensor([[0.0317, 0.9683]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[4.4420, 7.6870]], device='cuda:3') tensor([[0.0301, 0.9699]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[6.0076, 9.2137]], device='cuda:3') tensor([[0.0298, 0.9702]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[ 7.1179, 10.3164]], device='cuda:3') tensor([[0.0301, 0.9699]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[ 8.0144, 10.8765]], device='cuda:3') tensor([[0.0302, 0.9698]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[ 8.2399, 11.0836]], device='cuda:3') tensor([[0.0309, 0.9691]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[ 8.8255, 11.3833]], device='cuda:3') tensor([[0.0309, 0.9691]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[7.2412, 9.5508]], device='cuda:3') tensor([[0.0320, 0.9680]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[12.7171, 14.8376]], device='cuda:3') tensor([[0.0407, 0.9593]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[10.6774, 13.0917]], device='cuda:3') tensor([[0.0683, 0.9317]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[15.5961, 17.3087]], device='cuda:3') tensor([[0.2368, 0.7632]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[12.8510, 14.7162]], device='cuda:3') tensor([[0.2520, 0.7480]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[13.0397, 14.4773]], device='cuda:3') tensor([[0.9366, 0.0634]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[2.0184, 3.0302]], device='cuda:3') tensor([[0.9802, 0.0198]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-14.7485, -16.5789]], device='cuda:3') tensor([[0.9672, 0.0328]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-21.6351, -25.5703]], device='cuda:3') tensor([[0.9152, 0.0848]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "[EPIPSODE] 671 [DURATION] 27 [Q_LOSS] 19.556407048992586 [POLICY LOSS] 0.004343048203736544\n",
      "tensor([[6.0196, 8.5532]], device='cuda:3') tensor([[0.7445, 0.2555]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[6.0639, 8.5910]], device='cuda:3') tensor([[0.6482, 0.3518]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[2.3076, 5.1504]], device='cuda:3') tensor([[0.3421, 0.6579]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[1.9629, 5.0708]], device='cuda:3') tensor([[0.1568, 0.8432]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[4.0368, 7.1361]], device='cuda:3') tensor([[0.0801, 0.9199]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[5.0844, 8.2571]], device='cuda:3') tensor([[0.0932, 0.9068]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[ 9.9609, 12.4078]], device='cuda:3') tensor([[0.0937, 0.9063]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[10.0093, 12.4537]], device='cuda:3') tensor([[0.0915, 0.9085]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[ 9.8948, 12.3628]], device='cuda:3') tensor([[0.2026, 0.7974]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[12.6321, 14.4397]], device='cuda:3') tensor([[0.2765, 0.7235]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[7.2764, 8.9597]], device='cuda:3') tensor([[0.4978, 0.5022]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[5.7063, 7.3269]], device='cuda:3') tensor([[0.8464, 0.1536]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[3.6648, 5.1574]], device='cuda:3') tensor([[0.9811, 0.0189]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-3.4620, -3.0222]], device='cuda:3') tensor([[0.9875, 0.0125]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-6.2269, -6.3952]], device='cuda:3') tensor([[0.9877, 0.0123]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-4.1803, -4.2892]], device='cuda:3') tensor([[0.9879, 0.0121]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-4.3791, -4.8161]], device='cuda:3') tensor([[0.9879, 0.0121]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-2.2594, -2.1309]], device='cuda:3') tensor([[0.9879, 0.0121]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-0.2475,  0.7039]], device='cuda:3') tensor([[0.9875, 0.0125]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-0.7650,  0.3143]], device='cuda:3') tensor([[0.9863, 0.0137]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-3.4618, -2.9049]], device='cuda:3') tensor([[0.9734, 0.0266]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-5.2179, -4.4285]], device='cuda:3') tensor([[0.9536, 0.0464]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-1.2074, -0.0072]], device='cuda:3') tensor([[0.9139, 0.0861]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-2.2153, -0.4755]], device='cuda:3') tensor([[0.7927, 0.2073]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-3.0579, -1.2049]], device='cuda:3') tensor([[0.6761, 0.3239]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-3.4619, -1.4410]], device='cuda:3') tensor([[0.4820, 0.5180]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-4.7653, -2.3481]], device='cuda:3') tensor([[0.3812, 0.6188]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-2.9446, -0.9038]], device='cuda:3') tensor([[0.2037, 0.7963]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-5.3067, -2.5648]], device='cuda:3') tensor([[0.1150, 0.8850]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-6.2704, -4.1449]], device='cuda:3') tensor([[0.1076, 0.8924]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-11.7006, -10.1919]], device='cuda:3') tensor([[0.0861, 0.9139]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-12.3241, -11.0447]], device='cuda:3') tensor([[0.0952, 0.9048]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-18.2040, -17.1202]], device='cuda:3') tensor([[0.1140, 0.8860]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-17.3059, -16.3622]], device='cuda:3') tensor([[0.1398, 0.8602]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-17.7760, -16.9255]], device='cuda:3') tensor([[0.3952, 0.6048]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-23.4212, -23.3889]], device='cuda:3') tensor([[0.5397, 0.4603]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-17.2818, -17.2023]], device='cuda:3') tensor([[0.7893, 0.2107]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "[EPIPSODE] 672 [DURATION] 37 [Q_LOSS] 11.290523386342453 [POLICY LOSS] 0.0324573740363121\n",
      "tensor([[-5.5339, -5.1967]], device='cuda:3') tensor([[0.6091, 0.3909]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-5.4861, -5.1494]], device='cuda:3') tensor([[0.5139, 0.4861]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-4.2141, -3.2351]], device='cuda:3') tensor([[0.3620, 0.6380]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-4.8998, -3.6223]], device='cuda:3') tensor([[0.3475, 0.6525]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-6.5848, -5.7647]], device='cuda:3') tensor([[0.3373, 0.6627]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-7.1382, -6.3110]], device='cuda:3') tensor([[0.3648, 0.6352]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-7.0632, -6.2385]], device='cuda:3') tensor([[0.5558, 0.4442]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-9.9798, -9.6766]], device='cuda:3') tensor([[0.8201, 0.1799]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-12.9035, -13.0119]], device='cuda:3') tensor([[0.8442, 0.1558]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-8.3112, -8.6014]], device='cuda:3') tensor([[0.8390, 0.1610]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-6.2853, -6.7862]], device='cuda:3') tensor([[0.8247, 0.1753]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-6.0289, -6.5273]], device='cuda:3') tensor([[0.8146, 0.1854]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-5.6198, -6.1406]], device='cuda:3') tensor([[0.7152, 0.2848]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-5.1435, -5.1429]], device='cuda:3') tensor([[0.3881, 0.6119]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-6.8465, -5.6546]], device='cuda:3') tensor([[0.2864, 0.7136]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-7.6519, -6.8853]], device='cuda:3') tensor([[0.1513, 0.8487]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-9.2951, -8.3102]], device='cuda:3') tensor([[0.0868, 0.9132]], device='cuda:3', grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for episode in count():\n",
    "    complete = run_episode(episode, env, log)\n",
    "\n",
    "    if complete:\n",
    "        print('complete...!')\n",
    "        break\n",
    "    \n",
    "    if episode % 50 == 0:\n",
    "        clear_output(wait=True)\n",
    "        torch.save(policy_net.state_dict(), f'models/{name}_policy.pt')\n",
    "        torch.save(q_net.state_dict(), f'models/{name}_q.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frequent-extreme",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
