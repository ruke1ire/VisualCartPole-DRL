{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "entitled-physics",
   "metadata": {},
   "source": [
    "# Actor Critic with replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "signed-refund",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "from itertools import count\n",
    "from logger import Logger\n",
    "from pyvirtualdisplay import Display\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "passive-child",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyvirtualdisplay.display.Display at 0x7fb520080518>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "virtual_display = Display(visible=0, size=(1400, 900))\n",
    "virtual_display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "thorough-pixel",
   "metadata": {},
   "outputs": [],
   "source": [
    "############ HYPERPARAMETERS ##############\n",
    "FRAMES = 2\n",
    "RESIZE_PIXELS = 40\n",
    "\n",
    "device = 'cuda:3'\n",
    "\n",
    "POLICY_LR = 5e-6\n",
    "Q_LR = 1e-3\n",
    "GAMMA = 0.99\n",
    "BETA = 0.05\n",
    "END_SCORE = 1000\n",
    "MEMORY_SIZE = 150000\n",
    "BATCH_SIZE = 128\n",
    "############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "lovely-major",
   "metadata": {},
   "outputs": [],
   "source": [
    "############ ENVIRONMENT ##############\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(\"CartPole-v0\").unwrapped\n",
    "        self.env.reset()\n",
    "        \n",
    "        screen = self.env.render(mode='rgb_array').transpose((2,0,1))\n",
    "        _, self.screen_height, self.screen_width = screen.shape\n",
    "        \n",
    "        self.resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(RESIZE_PIXELS, interpolation=Image.CUBIC),\n",
    "                    T.Grayscale(),\n",
    "                    T.ToTensor()])\n",
    "        \n",
    "        world_width = self.env.x_threshold * 2\n",
    "        self.scale = self.screen_width / world_width\n",
    "        \n",
    "    def get_cart_location(self):\n",
    "        return int(self.env.state[0] * self.scale + self.screen_width / 2.0)\n",
    "        \n",
    "    def get_screen(self):\n",
    "        screen = self.env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "        \n",
    "        screen = screen[:, int(self.screen_height*0.4):int(self.screen_height * 0.8)]\n",
    "        screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "        screen = torch.from_numpy(screen)\n",
    "        return self.resize(screen).unsqueeze(0)\n",
    "    \n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "        \n",
    "    def step(self, action):\n",
    "        return self.env.step(action)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adverse-quality",
   "metadata": {},
   "outputs": [],
   "source": [
    "############ MEMORY ##############\n",
    "\n",
    "Transition = namedtuple('Transition',('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity,memory=None):\n",
    "        self.capacity = capacity\n",
    "        if memory != None:\n",
    "            self.memory = memory\n",
    "        else:\n",
    "            self.memory = []\n",
    "            \n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "choice-uniform",
   "metadata": {},
   "outputs": [],
   "source": [
    "############ NETWORK ##############\n",
    "'''\n",
    "For policy gradient, the network should input the raw pixels, and output the probabilities of choosing either 0 or 1\n",
    "'''\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self,h=60,w=135,device='cuda:0'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        input_channel = 2\n",
    "        hidden_channel = 32#64\n",
    "        hidden_channel2 = 32\n",
    "        kernel_size = 5\n",
    "        stride = 2\n",
    "        \n",
    "        self.base = nn.Sequential(\n",
    "            nn.Conv2d(input_channel, hidden_channel, kernel_size = kernel_size, stride = stride),\n",
    "            nn.BatchNorm2d(hidden_channel),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_channel, hidden_channel, kernel_size = kernel_size, stride = stride),\n",
    "            nn.BatchNorm2d(hidden_channel),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_channel, hidden_channel2, kernel_size = kernel_size, stride = stride),\n",
    "            nn.BatchNorm2d(hidden_channel2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        def conv2d_size_out(size, kernel_size = kernel_size, stride = stride):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        \n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * hidden_channel2\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(linear_input_size, 24),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(24, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = x.to(self.device)\n",
    "        x = self.base(x)\n",
    "        x = x.flatten(1)\n",
    "        x = self.head(x)\n",
    "        return F.softmax(x,dim = 1)\n",
    "    \n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self,h=60,w=135,device='cuda:0'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        input_channel = 2\n",
    "        hidden_channel = 128#64\n",
    "        hidden_channel2 = 32\n",
    "        kernel_size = 5\n",
    "        stride = 2\n",
    "        \n",
    "        self.base = nn.Sequential(\n",
    "            nn.Conv2d(input_channel, hidden_channel, kernel_size = kernel_size, stride = stride),\n",
    "            nn.BatchNorm2d(hidden_channel),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_channel, hidden_channel, kernel_size = kernel_size, stride = stride),\n",
    "            nn.BatchNorm2d(hidden_channel),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_channel, hidden_channel2, kernel_size = kernel_size, stride = stride),\n",
    "            nn.BatchNorm2d(hidden_channel2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        def conv2d_size_out(size, kernel_size = kernel_size, stride = stride):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        \n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * hidden_channel2\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(linear_input_size, 24),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(24, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = x.to(self.device)\n",
    "        x = self.base(x)\n",
    "        x = x.flatten(1)\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "excited-snowboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "portable-democracy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.step(0)\n",
    "plt.figure()\n",
    "plt.imshow(env.get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy().squeeze(), cmap='gray')\n",
    "plt.title('Example extracted screen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "manual-cisco",
   "metadata": {},
   "outputs": [],
   "source": [
    "screen_shape = env.get_screen().shape\n",
    "policy_net = PolicyNetwork(h = screen_shape[2], w = screen_shape[3], device = device).to(device)\n",
    "\n",
    "q_net = QNetwork(h = screen_shape[2], w = screen_shape[3], device = device).to(device)\n",
    "\n",
    "policy_optimizer = optim.Adam(policy_net.parameters(), lr=POLICY_LR, weight_decay=1e-4)\n",
    "q_optimizer = optim.Adam(q_net.parameters(), lr = Q_LR, weight_decay = 1e-4)\n",
    "\n",
    "q_mse_loss = nn.MSELoss()\n",
    "\n",
    "memory = ReplayMemory(MEMORY_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "extensive-arabic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Network output: tensor([[0.6050, 0.3950]], device='cuda:1', grad_fn=<SoftmaxBackward>)\n",
      "Q Network output: tensor([[-0.1001,  0.1641]], device='cuda:1', grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "p_screen = env.get_screen().to(device)\n",
    "c_screen = env.get_screen().to(device)\n",
    "\n",
    "x = torch.cat((p_screen, c_screen),dim=1)\n",
    "\n",
    "print(\"Policy Network output:\",policy_net(x))\n",
    "print(\"Q Network output:\", q_net(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "illegal-psychology",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(episode, env, logger = None):\n",
    "    policy_net.eval()\n",
    "    q_net.eval()\n",
    "    \n",
    "    state = env.reset()\n",
    "    prev_screen = env.get_screen().to(device)\n",
    "    prev_x = None\n",
    "    prev_y = None\n",
    "    \n",
    "    reward_sum = 0\n",
    "\n",
    "    for steps in count():\n",
    "        screen = env.get_screen().to(device)\n",
    "        \n",
    "        x = torch.cat((prev_screen, screen), dim=1)\n",
    "        \n",
    "        action_prob = policy_net(x)\n",
    "        action = 0 if random.random() < action_prob[0][0] else 1\n",
    "\n",
    "        y = torch.tensor([[1, 0]] if action == 0 else [[0, 1]],dtype=torch.int64).to(device)\n",
    "        \n",
    "        state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            x = None\n",
    "            reward = -30\n",
    "        \n",
    "        reward = torch.tensor([reward], dtype=float).to(device)\n",
    "        \n",
    "        if prev_x is not None:\n",
    "            memory.push(prev_x, prev_y, x, reward)\n",
    "            policy_loss, entropy = update_policy(prev_x, prev_y, q_net, policy_net, policy_optimizer)\n",
    "            #print(q_net(prev_x).detach(), action_prob)\n",
    "        \n",
    "        reward_sum += reward\n",
    "        prev_screen = screen\n",
    "        prev_x = x\n",
    "        prev_y = y\n",
    "        \n",
    "        if steps % 10 == 0:\n",
    "            q_loss = update_q_batch(memory, q_net, policy_net, q_optimizer)\n",
    "            \n",
    "        if done or (steps == 1000):\n",
    "            if logger != None:\n",
    "                log.log_scalar(scalar=steps,episode=episode,name='duration')\n",
    "                log.log_scalar(scalar=reward_sum,episode=episode,name='reward')\n",
    "                log.log_scalar(scalar=entropy,episode=episode,name='entropy')\n",
    "                log.log_scalar(scalar=q_loss,episode=episode,name='q_loss')\n",
    "                log.log_scalar(scalar=policy_loss,episode=episode,name='policy_loss')\n",
    "            print(f\"[EPIPSODE] {episode} [DURATION] {steps} [Q_LOSS] {q_loss} [POLICY LOSS] {policy_loss}\")\n",
    "            return False, steps\n",
    "                \n",
    "def update_q_batch(memory, q_net, policy_net, optimizer):\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return 0\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    \n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    \n",
    "    state_action_values = torch.sum(q_net(state_batch) * action_batch,dim=1).to(torch.float64)\n",
    "\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    \n",
    "    next_state_values[non_final_mask] = torch.sum(q_net(non_final_next_states).detach() * policy_net(non_final_next_states).detach(),dim=1)\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    loss = F.mse_loss(state_action_values, expected_state_action_values)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "def update_policy(state, action, q_net, policy_net, optimizer):\n",
    "    \n",
    "    # policy_net = policy_net - lr * delta(log(policy_net) * (q_net(s,a) - q_net(s) @ policy_net(s)))\n",
    "    \n",
    "    policy_net.train()\n",
    "    \n",
    "    action_prob = policy_net(state)\n",
    "    q = q_net(state).detach()\n",
    "    \n",
    "    q_policy = q@action_prob.detach().T\n",
    "    q_actual = q@action.float().T\n",
    "    adv = q_actual - q_policy\n",
    "    \n",
    "    log_action_prob = torch.log(action_prob)\n",
    "    log_lik = action.float()@log_action_prob.T\n",
    "    log_lik_adv = log_lik * adv\n",
    "    \n",
    "    entropy = -log_action_prob@action_prob.T\n",
    "    \n",
    "    loss = -log_lik_adv - BETA*entropy\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item(), entropy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "piano-gravity",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = f\"AC-RM-2 Q_LR={Q_LR} P_LR={POLICY_LR} BATCH_SIZE={BATCH_SIZE} GAMMA={GAMMA} MEMORY_SIZE={MEMORY_SIZE} BETA={BETA}\"\n",
    "log = Logger(model_name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-siemens",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPIPSODE] 301 [DURATION] 11 [Q_LOSS] 16.393779496800963 [POLICY LOSS] -0.015452123247087002\n",
      "[EPIPSODE] 302 [DURATION] 8 [Q_LOSS] 11.459747293075061 [POLICY LOSS] -0.008439529687166214\n",
      "[EPIPSODE] 303 [DURATION] 32 [Q_LOSS] 13.312085025966306 [POLICY LOSS] -0.07169903814792633\n",
      "[EPIPSODE] 304 [DURATION] 16 [Q_LOSS] 16.23813404317645 [POLICY LOSS] 0.008971752598881721\n",
      "[EPIPSODE] 305 [DURATION] 17 [Q_LOSS] 13.979012517817736 [POLICY LOSS] -0.07436752319335938\n",
      "[EPIPSODE] 306 [DURATION] 9 [Q_LOSS] 6.976629126363143 [POLICY LOSS] -0.9813740849494934\n",
      "[EPIPSODE] 307 [DURATION] 32 [Q_LOSS] 13.986532008251297 [POLICY LOSS] -0.08871573954820633\n",
      "[EPIPSODE] 308 [DURATION] 25 [Q_LOSS] 14.978809371188355 [POLICY LOSS] -0.10688222199678421\n",
      "[EPIPSODE] 309 [DURATION] 39 [Q_LOSS] 30.346132361334654 [POLICY LOSS] 0.12249387800693512\n",
      "[EPIPSODE] 310 [DURATION] 12 [Q_LOSS] 11.717418038487443 [POLICY LOSS] 0.4947572350502014\n",
      "[EPIPSODE] 311 [DURATION] 23 [Q_LOSS] 11.530854182869781 [POLICY LOSS] 0.02607610821723938\n",
      "[EPIPSODE] 312 [DURATION] 10 [Q_LOSS] 20.546218141378922 [POLICY LOSS] 0.07892249524593353\n",
      "[EPIPSODE] 313 [DURATION] 27 [Q_LOSS] 13.764597955009584 [POLICY LOSS] -0.5823936462402344\n",
      "[EPIPSODE] 314 [DURATION] 38 [Q_LOSS] 13.702116353580204 [POLICY LOSS] -0.4042154550552368\n",
      "[EPIPSODE] 315 [DURATION] 36 [Q_LOSS] 17.378465113415974 [POLICY LOSS] -0.36486247181892395\n",
      "[EPIPSODE] 316 [DURATION] 10 [Q_LOSS] 32.67327092334747 [POLICY LOSS] 0.029560483992099762\n",
      "[EPIPSODE] 317 [DURATION] 66 [Q_LOSS] 20.914234388233258 [POLICY LOSS] 0.030902313068509102\n",
      "[EPIPSODE] 318 [DURATION] 16 [Q_LOSS] 12.166187264979026 [POLICY LOSS] 0.07235027849674225\n",
      "[EPIPSODE] 319 [DURATION] 41 [Q_LOSS] 18.83383100559138 [POLICY LOSS] 0.22264087200164795\n",
      "[EPIPSODE] 320 [DURATION] 33 [Q_LOSS] 13.47682687420091 [POLICY LOSS] 0.023565787822008133\n",
      "[EPIPSODE] 321 [DURATION] 10 [Q_LOSS] 17.95512738157225 [POLICY LOSS] 0.1135103851556778\n",
      "[EPIPSODE] 322 [DURATION] 36 [Q_LOSS] 12.044459001688592 [POLICY LOSS] -0.1521930694580078\n",
      "[EPIPSODE] 323 [DURATION] 9 [Q_LOSS] 10.831069295673293 [POLICY LOSS] 0.02116754837334156\n",
      "[EPIPSODE] 324 [DURATION] 32 [Q_LOSS] 27.416508438227787 [POLICY LOSS] -0.0205516554415226\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for episode in count():\n",
    "    complete, step = run_episode(episode, env, log)\n",
    "\n",
    "    if complete:\n",
    "        print('complete...!')\n",
    "        break\n",
    "    \n",
    "    if episode % 50 == 0:\n",
    "        clear_output(wait=True)\n",
    "        torch.save(policy_net.state_dict(), f'models/{name}_policy.pt')\n",
    "        torch.save(q_net.state_dict(), f'models/{name}_q.pt')\n",
    "        \n",
    "    if step == 1000:\n",
    "        torch.save(policy_net.state_dict(), f'models/{name}_policy_{episode}.pt')\n",
    "        torch.save(q_net.state_dict(), f'models/{name}_q_{episode}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "japanese-headline",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
